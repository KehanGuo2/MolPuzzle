{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IR question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=6):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "path = \"OpenGVLab/InternVL-Chat-V1-5\"\n",
    "# If you have an 80G A100 GPU, you can put the entire model on a single GPU.\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True, cache_dir=\"/scratch365/kguo2/TRANS_cache/\").eval().cuda()\n",
    "# Otherwise, you need to set device_map='auto' to use multiple GPUs for inference.\n",
    "# import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map='auto').eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "\n",
    "generation_config = dict(\n",
    "    num_beams=1,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# single-round single-image conversation\n",
    "question = \"请详细描述图片\" # Please describe the picture in detail\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(question, response)\n",
    "\n",
    "# # multi-round single-image conversation\n",
    "# question = \"请详细描述图片\" # Please describe the picture in detail\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "# print(question, response)\n",
    "\n",
    "# question = \"请根据图片写一首诗\" # Please write a poem according to the picture\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "# print(question, response)\n",
    "\n",
    "# # multi-round multi-image conversation\n",
    "# pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "# pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "# pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "# question = \"详细描述这两张图片\" # Describe the two pictures in detail\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "# print(question, response)\n",
    "\n",
    "# question = \"这两张图片的相同点和区别分别是什么\" # What are the similarities and differences between these two pictures\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "# print(question, response)\n",
    "\n",
    "# # batch inference (single image per sample)\n",
    "# pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "# pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "# image_counts = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "# pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "# questions = [\"Describe the image in detail.\"] * len(image_counts)\n",
    "# responses = model.batch_chat(tokenizer, pixel_values,\n",
    "#                              image_counts=image_counts,\n",
    "#                              questions=questions,\n",
    "#                              generation_config=generation_config)\n",
    "# for question, response in zip(questions, responses):\n",
    "#     print(question)\n",
    "#     print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./data/mol_figures/step2/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Define the functional groups for questions that require iterating over different groups\n",
    "# Alcohol,Phenol,Ketone,Acid,Aldehyde,Ester,Aromatic,Nitrile,Isocyanate,Amine,Amide,Ether,Sulfide,Halide,Alkyl,Nitric oxide,Acetaldehyde,Benzyl,Iodine,Ethyl,Hydroxymethyl ,Nitro ,Benzene,Benzaldehyde,Alkyne ,C-O Stretching,C=O Stretching,O-H Stretching,N-H Stretching,triple bond C-H Stretching:,Degree_Unsaturation,Saturation\n",
    "\n",
    "functional_groups = [\n",
    "    \"Alcohol\", \"Phenol\", \"Ketone\", \"Acid\", \"Aldehyde\", \"Ester\", \"Nitrile\",\n",
    "    \"Isocyanate\", \"Amine\", \"Ether\", \"Sulfide\", \"Halide\"]\n",
    "\n",
    "data = pd.read_csv(\"./data/mol_figures/step2.csv\")\n",
    "# Create a new DataFrame to store the generated questions and answers\n",
    "columns = [\"Molecule Index\", \"SMILES\",\"cls\", \"Formula\", \"Question\", \"Answer\"]\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Generate questions and answers based on the template provided\n",
    "for _, row in data.iterrows():\n",
    "    #   Q31: O-H Stretching question\n",
    "    oh_question = \"Does the IR spectrum contains broad absorption peak of O-H stretching around 3300 cm⁻1?\"\n",
    "    oh_answer = \"Yes\" if row['O-H Stretching'] == 1 else \"No\"\n",
    "    qclass = \"IR spectrum peak analysis\"\n",
    "    results_df.loc[len(results_df)] = [row['Molecule Index'], row['SMILES'], qclass, row['Formula'], oh_question, oh_answer]\n",
    "\n",
    "    # Q32: alkyl question\n",
    "    akl_question = f\"Does the IR spectrum contains sharp absorption peak of Alkyl stretching around 2900 cm⁻1?\"\n",
    "    akl_answer = \"Yes\" if row['Alkyl'] == 1 else \"No\"\n",
    "    qclass = \"IR spectrum peak analysis\" \n",
    "    results_df.loc[len(results_df)] = [row['Molecule Index'], row['SMILES'], qclass,row['Formula'], akl_question, akl_answer]\n",
    "\n",
    "    #Q33 C=O Stretching question\n",
    "    co_question = f\" Does the IR spectrum contains strong, sharp peak of C=O stretching around 1700 cm⁻¹?\"\n",
    "    co_answer = \"Yes\" if row['C=O Stretching'] == 1 else \"No\"\n",
    "    qclass = \"IR spectrum peak analysis\" \n",
    "    results_df.loc[len(results_df)] = [row['Molecule Index'], row['SMILES'], qclass,row['Formula'], co_question, co_answer]\n",
    "\n",
    "    #Q34 N-H Stretching question\n",
    "    nh_question = f\"Does the IR spectrum contains broad absorption peak of N-H stretching around 3200-3600 cm⁻1?\"\n",
    "    nh_answer = \"Yes\" if row['N-H Stretching'] == 1 else \"No\"\n",
    "    qclass = \"IR spectrum peak analysis\"\n",
    "    results_df.loc[len(results_df)] = [row['Molecule Index'], row['SMILES'],qclass, row['Formula'], nh_question, nh_answer]\n",
    "\n",
    "    #Q35 triple bond C-H Stretching question\n",
    "    tbch_question = f\"Does the IR spectrum contains weak absorption peak of triple bond C-H stretching around 2260-2100 cm⁻1?\"\n",
    "    tbch_answer = \"Yes\" if row['triple bond C-H Stretching:'] == 1 else \"No\"\n",
    "    qclass = \"IR spectrum peak analysis\"\n",
    "    results_df.loc[len(results_df)] = [row['Molecule Index'], row['SMILES'],qclass, row['Formula'], tbch_question, tbch_answer]\n",
    "\n",
    "\n",
    "    # Q17: Functional group presence question\n",
    "    for group in functional_groups:\n",
    "        # Q1: Basic functional group presence question\n",
    "        question = f\" Examine the IR spectrum to determine if the molecule could potentially contain specific functional groups: {group}? Look for the presence of characteristic absorption bands and analyze the wavenumbers and intensities of these peaks. This analysis will help identify the functional groups and key structural features within the molecule.\"\n",
    "        answer = \"Yes\" if row[group] == 1 else \"No\"\n",
    "        qclass = \"IR spectrum structure elucidation\"\n",
    "        results_df.loc[len(results_df)] = [row['Molecule Index'], row['SMILES'],qclass, row['Formula'], question, answer]\n",
    "\n",
    "\n",
    "results_df.to_csv(\"./data/mol_figures/step2/IR_questions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MASS question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "functional_groups = [\n",
    "    \"Alcohol\", \"Phenol\", \"Ketone\", \"Acid\", \"Aldehyde\", \"Ester\", \"Nitrile\",\n",
    "    \"Isocyanate\", \"Amine\", \"Ether\", \"Sulfide\", \"Halide\"]\n",
    "data = pd.read_csv(\"./data/mol_figures/step2.csv\")\n",
    "# Create a new DataFrame to store the generated questions and answers\n",
    "columns = [\"Molecule Index\", \"SMILES\",\"cls\", \"Formula\", \"Question\", \"Answer\"]\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Generate questions and answers based on the template provided\n",
    "for _, row in data.iterrows():\n",
    "        for group in functional_groups:\n",
    "        # Q1: Basic functional group presence question\n",
    "            question = f\"Examine the MS spectrum to determine if the molecule could potentially contain specific fragments:{group}. Look into the number of fragments observed and analyze the differences between the larger fragments. This analysis will help identify the presence of key structural features within the molecule?\"\n",
    "            answer = \"Yes\" if row[group] == 1 else \"No\"\n",
    "            qclass = \"MASS spectrum structure elucidation\"\n",
    "            results_df.loc[len(results_df)] = [row['Molecule Index'], row['SMILES'],qclass, row['Formula'], question, answer]\n",
    "\n",
    "results_df.to_csv(\"./data/mol_figures/step2/MASS_questions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Examine the MS spectrum to determine if the molecule could potentially contain specific fragments:Alcohol. Look into the number of fragments observed and analyze the differences between the larger fragments. This analysis will help identify the presence of key structural features within the molecule?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['Question'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iteration = 3\n",
    "data = pd.read_csv(\"./data/mol_figures/step2/MASS_questions.csv\")\n",
    "\n",
    "ratios = {\n",
    "        'MASS spectrum structure elucidation': 1,\n",
    "    }\n",
    "for i in range(0, iteration):\n",
    "    total_samples = 100\n",
    "    samples_per_class = {clss: int(total_samples * ratio) for clss, ratio in ratios.items()}\n",
    "    print(samples_per_class)\n",
    "\n",
    "    sampled_data = pd.DataFrame()\n",
    "    for clss, n_samples in samples_per_class.items():\n",
    "        print(clss)\n",
    "        sampled_class_data = data[data['cls'] == clss].sample(n=n_samples)\n",
    "        sampled_data = pd.concat([sampled_data, sampled_class_data])\n",
    "\n",
    "    if len(sampled_data) < total_samples:\n",
    "        additional_samples = data[~data.index.isin(sampled_data.index)].sample(n=total_samples - len(sampled_data), random_state=42)\n",
    "        sampled_data = pd.concat([sampled_data, additional_samples])\n",
    "    # os.makedirs('./data/mol_figures/mol_understanding', exist_ok=True)\n",
    "    sampled_data.to_csv(f'./data/mol_figures/step2/MASS_sampled_questions_answers_{i}.csv', index=False)\n",
    "print(\"Sampled data saved to 'MASS_sampled_questions_answers.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IR spectrum peak analysis': 80, 'IR spectrum structure elucidation': 20}\n",
      "IR spectrum peak analysis\n",
      "IR spectrum structure elucidation\n",
      "Sampled data saved to 'IR_sampled_questions_answers.csv'\n",
      "{'IR spectrum peak analysis': 80, 'IR spectrum structure elucidation': 20}\n",
      "IR spectrum peak analysis\n",
      "IR spectrum structure elucidation\n",
      "Sampled data saved to 'IR_sampled_questions_answers.csv'\n",
      "{'IR spectrum peak analysis': 80, 'IR spectrum structure elucidation': 20}\n",
      "IR spectrum peak analysis\n",
      "IR spectrum structure elucidation\n",
      "Sampled data saved to 'IR_sampled_questions_answers.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "iteration = 3\n",
    "data = pd.read_csv(\"./data/mol_figures/step2/IR_questions.csv\")\n",
    "ratios = {\n",
    "        'IR spectrum peak analysis': 0.8,\n",
    "        'IR spectrum structure elucidation': 0.2,\n",
    "    }\n",
    "for i in range(0, iteration):\n",
    "    total_samples = 100\n",
    "    samples_per_class = {clss: int(total_samples * ratio) for clss, ratio in ratios.items()}\n",
    "    print(samples_per_class)\n",
    "\n",
    "    sampled_data = pd.DataFrame()\n",
    "    for clss, n_samples in samples_per_class.items():\n",
    "        print(clss)\n",
    "        sampled_class_data = data[data['cls'] == clss].sample(n=n_samples)\n",
    "        sampled_data = pd.concat([sampled_data, sampled_class_data])\n",
    "\n",
    "\n",
    "    if len(sampled_data) < total_samples:\n",
    "        additional_samples = data[~data.index.isin(sampled_data.index)].sample(n=total_samples - len(sampled_data), random_state=42)\n",
    "        sampled_data = pd.concat([sampled_data, additional_samples])\n",
    "    # os.makedirs('./data/mol_figures/mol_understanding', exist_ok=True)\n",
    "    sampled_data.to_csv(f'./data/mol_figures/step2/IR_sampled_questions_answers_{i}.csv', index=False)\n",
    "    print(\"Sampled data saved to 'IR_sampled_questions_answers.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MASS spectrum structure elucidation': 100}\n",
      "MASS spectrum structure elucidation\n",
      "Sampled data saved to 'IR_sampled_questions_answers.csv'\n",
      "{'MASS spectrum structure elucidation': 100}\n",
      "MASS spectrum structure elucidation\n",
      "Sampled data saved to 'IR_sampled_questions_answers.csv'\n",
      "{'MASS spectrum structure elucidation': 100}\n",
      "MASS spectrum structure elucidation\n",
      "Sampled data saved to 'IR_sampled_questions_answers.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "iteration = 3\n",
    "data = pd.read_csv(\"./data/mol_figures/step2/MASS_questions.csv\")\n",
    "\n",
    "ratios = {\n",
    "        'MASS spectrum structure elucidation': 1,\n",
    "    }\n",
    "for i in range(0, iteration):\n",
    "    total_samples = 100\n",
    "    samples_per_class = {clss: int(total_samples * ratio) for clss, ratio in ratios.items()}\n",
    "    print(samples_per_class)\n",
    "\n",
    "    sampled_data = pd.DataFrame()\n",
    "    for clss, n_samples in samples_per_class.items():\n",
    "        print(clss)\n",
    "        sampled_class_data = data[data['cls'] == clss].sample(n=n_samples)\n",
    "        sampled_data = pd.concat([sampled_data, sampled_class_data])\n",
    "\n",
    "\n",
    "    if len(sampled_data) < total_samples:\n",
    "        additional_samples = data[~data.index.isin(sampled_data.index)].sample(n=total_samples - len(sampled_data), random_state=42)\n",
    "        sampled_data = pd.concat([sampled_data, additional_samples])\n",
    "    # os.makedirs('./data/mol_figures/mol_understanding', exist_ok=True)\n",
    "    sampled_data.to_csv(f'./data/mol_figures/step2/IR_sampled_questions_answers_{i}.csv', index=False)\n",
    "    print(\"Sampled data saved to 'IR_sampled_questions_answers.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero-shotting learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Examine the MS spectrum to determine if the molecule could potentially contain specific fragments:Alcohol. Look into the number of fragments observed and analyze the differences between the larger fragments. This analysis will help identify the presence of key structural features within the molecule?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data['Question'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "import torch\n",
    "from utils import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from transformers.generation import GenerationConfig\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ['ANTHROPIC_API_KEY'] = ''\n",
    "cache_dir = \"\"\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "    As an expert organic chemist, your task is to analyze and determine the potential structures that can be derived from a given molecular MASS spectrum image.\\\n",
    "    Utilize your knowledge to systematically explore and identify plausible structural configurations based on the MASS spectrum image provided and answer the question.\\\n",
    "    Analyze the problem step-by-step internally, but do not include the analysis in your output. Provide only a very short answer with the exact result. Respond with ‘Yes’ or ‘No’ to the question.\n",
    "    example: output:Yes\n",
    "    \"\"\"\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def analyze_image_with_prompt_gpt(image_paths, prompt_text, sys_prompt=sys_prompt):\n",
    "    # Encode the image to base64\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    base64_images = [encode_image(image_path) for image_path in image_paths]\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": sys_prompt + \"\\n\" + prompt_text\n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "      ]\n",
    "    \n",
    "    for base64_image in base64_images:\n",
    "        messages[0]['content'].append(\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages,\n",
    "      max_tokens=500,\n",
    "      temperature=0.7\n",
    "    )\n",
    "    \n",
    "    gpt_responses = response.choices[0]\n",
    "    \n",
    "    # Return only the GPT model's responses\n",
    "    return gpt_responses\n",
    "\n",
    "def analyze_image_with_prompt_claude(image_paths, prompt_text, sys_prompt=sys_prompt):\n",
    "    \n",
    "    # Encode the image to base64\n",
    "    api_key = ''\n",
    "    client = anthropic.Anthropic(api_key=api_key)\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": 'image/png',\n",
    "                        \"data\": base64_image,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image.\"\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    )\n",
    "  \n",
    "    return message\n",
    "\n",
    "\n",
    "def analyze_image_with_prompt_llava_8B(model, processor, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    images = Image.open(image_path)\n",
    "\n",
    "    prompt = (f\"<|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n<image>\\n{prompt_text}<|eot_id|>\"\n",
    "              \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "    inputs = processor(prompt, images, return_tensors='pt').to(0, torch.float16)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, do_sample=False)\n",
    "    len_tokens = len(prompt.split())\n",
    "    generated_text = processor.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "    \n",
    "def analyze_image_with_prompt_instructBlip(model, processor, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    if image_path is not None:\n",
    "        images = Image.open(image_path).convert(\"RGB\")\n",
    "        prompt = prompt_text\n",
    "        inputs = processor(images=images, text='You are an expert in organic chemistry, based on the provided image, answer the following question. ' + prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    else:\n",
    "        prompt = prompt_text\n",
    "        inputs = processor(text=sys_prompt + prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            num_beams=5,\n",
    "            max_length=512,\n",
    "            min_length=1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.5,\n",
    "            length_penalty=1.0,\n",
    "            temperature=1,\n",
    "    )\n",
    "    generated_text = processor.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return generated_text\n",
    "\n",
    "def analyze_image_with_prompt_Qwen(model, tokenizer, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    query = tokenizer.from_list_format([\n",
    "        {'image': image_path},\n",
    "        {'text': sys_prompt+prompt_text},\n",
    "    ])\n",
    "    response, history = model.chat(tokenizer, query=query, history=None)\n",
    "    return response\n",
    "    # # 2nd dialogue turn\n",
    "    # response, history = model.chat(tokenizer, '输出\"击掌\"的检测框', history=history)\n",
    "    # print(response)\n",
    "    # # <ref>击掌</ref><box>(517,508),(589,611)</box>\n",
    "    # image = tokenizer.draw_bbox_on_latest_picture(response, history)\n",
    "    # if image:\n",
    "    #   image.save('1.jpg')\n",
    "    # else:\n",
    "    #   print(\"no box\")\n",
    "\n",
    "def analyze_image_with_prompt_InternVL(model, tokenizer, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "    IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "    \n",
    "    \n",
    "    def build_transform(input_size):\n",
    "        MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "        transform = T.Compose([\n",
    "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "        return transform\n",
    "    \n",
    "    \n",
    "    def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "        best_ratio_diff = float('inf')\n",
    "        best_ratio = (1, 1)\n",
    "        area = width * height\n",
    "        for ratio in target_ratios:\n",
    "            target_aspect_ratio = ratio[0] / ratio[1]\n",
    "            ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "            if ratio_diff < best_ratio_diff:\n",
    "                best_ratio_diff = ratio_diff\n",
    "                best_ratio = ratio\n",
    "            elif ratio_diff == best_ratio_diff:\n",
    "                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                    best_ratio = ratio\n",
    "        return best_ratio\n",
    "    \n",
    "    \n",
    "    def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "        orig_width, orig_height = image.size\n",
    "        aspect_ratio = orig_width / orig_height\n",
    "    \n",
    "        # calculate the existing image aspect ratio\n",
    "        target_ratios = set(\n",
    "            (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "            i * j <= max_num and i * j >= min_num)\n",
    "        target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "    \n",
    "        # find the closest aspect ratio to the target\n",
    "        target_aspect_ratio = find_closest_aspect_ratio(\n",
    "            aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "    \n",
    "        # calculate the target width and height\n",
    "        target_width = image_size * target_aspect_ratio[0]\n",
    "        target_height = image_size * target_aspect_ratio[1]\n",
    "        blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "    \n",
    "        # resize the image\n",
    "        resized_img = image.resize((target_width, target_height))\n",
    "        processed_images = []\n",
    "        for i in range(blocks):\n",
    "            box = (\n",
    "                (i % (target_width // image_size)) * image_size,\n",
    "                (i // (target_width // image_size)) * image_size,\n",
    "                ((i % (target_width // image_size)) + 1) * image_size,\n",
    "                ((i // (target_width // image_size)) + 1) * image_size\n",
    "            )\n",
    "            # split the image\n",
    "            split_img = resized_img.crop(box)\n",
    "            processed_images.append(split_img)\n",
    "        assert len(processed_images) == blocks\n",
    "        if use_thumbnail and len(processed_images) != 1:\n",
    "            thumbnail_img = image.resize((image_size, image_size))\n",
    "            processed_images.append(thumbnail_img)\n",
    "        return processed_images\n",
    "    \n",
    "    \n",
    "    def load_image(image_file, input_size=448, max_num=6):\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        transform = build_transform(input_size=input_size)\n",
    "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(image) for image in images]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        return pixel_values\n",
    "    \n",
    "    \n",
    "    \n",
    "    # set the max number of tiles in `max_num`\n",
    "    pixel_values = load_image(image_path, max_num=6).to(torch.bfloat16).cuda()\n",
    "    \n",
    "    generation_config = dict(\n",
    "        num_beams=1,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    # single-round single-image conversation\n",
    "    question = sys_prompt + prompt_text\n",
    "    response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "    return response\n",
    "    \n",
    "\n",
    "def analyze_image_with_prompt(model_name, image_path, prompt_text, sys_prompt=sys_prompt, model=None, processor=None):\n",
    "    if 'gpt-4o' in model_name:\n",
    "        return analyze_image_with_prompt_gpt(image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif \"claude\" in model_name:\n",
    "        return analyze_image_with_prompt_claude(image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif \"llava\" in model_name:\n",
    "        return analyze_image_with_prompt_llava_8B(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif \"instructBlip\" in model_name:\n",
    "        return analyze_image_with_prompt_instructBlip(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif 'Qwen' in model_name:\n",
    "        return analyze_image_with_prompt_Qwen(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif 'InternVL' in model_name:\n",
    "        return analyze_image_with_prompt_InternVL(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose 'gpt' or 'claude'.\")\n",
    "    \n",
    "    \n",
    "def is_open_source(model_name):\n",
    "    if 'claude' in model_name or 'gemini' in model_name or 'gpt' in model_name:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iteration = 3\n",
    "model_names = ['llava', 'instructBlip-7B', 'instructBlip-13B', 'Qwen-VL-Chat']\n",
    "for model_name in model_names:\n",
    "    if is_open_source(model_name):\n",
    "        if 'llava' in model_name:\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                model_paths[model_name], \n",
    "                torch_dtype=torch.float16, \n",
    "                low_cpu_mem_usage=True, \n",
    "                cache_dir=cache_dir,\n",
    "                ).to(0)\n",
    "\n",
    "            processor = AutoProcessor.from_pretrained(model_paths[model_name])\n",
    "        elif 'instructBlip' in model_name:\n",
    "            model = InstructBlipForConditionalGeneration.from_pretrained(model_paths[model_name], cache_dir=cache_dir).to(\"cuda\")\n",
    "            processor = InstructBlipProcessor.from_pretrained(model_paths[model_name], cache_dir=cache_dir) \n",
    "        elif 'Qwen' in model_name:\n",
    "            processor = AutoTokenizer.from_pretrained(model_paths[model_name], trust_remote_code=True, cache_dir=cache_dir)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_paths[model_name], device_map=\"cuda\", trust_remote_code=True, cache_dir=cache_dir).eval()\n",
    "        elif 'InternVL' in model_name:\n",
    "            os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_paths[model_name],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=cache_dir,\n",
    "                device_map='auto').eval()\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, cache_dir=cache_dir)\n",
    "        else:\n",
    "            model = None\n",
    "            processor=None\n",
    "            \n",
    "    for i in range(0, iteration):\n",
    "        data_frame = pd.DataFrame(columns=[\"question\", \"cls\", \"Answer\", \"Generated Response\"])\n",
    "        data = pd.read_csv(f\"./data/mol_figures/step2/IR_sampled_questions_answers_{i}.csv\")\n",
    "        for _, row in data.iterrows():\n",
    "            try:\n",
    "                index_parts = row['Molecule Index'].split('_')\n",
    "                index_parts = [part.strip() for part in index_parts]\n",
    "                # image_path = \"/home/kguo2/PycharmProjects/spectrumLM/data/mol_figures/5th_specs/Problem_21_1.png\"\n",
    "                image_path = f'./data/mol_figures/{index_parts[0]}th_specs/Problem {index_parts[1]}_1.png'\n",
    "\n",
    "                generated_response = analyze_image_with_prompt(model_name, image_path, row['Question'], sys_prompt=sys_prompt, model=model, processor=processor)\n",
    "                # print(generated_response)\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'], row['Answer'], generated_response]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        data_frame.to_csv(f'./data/mol_figures/step2/IR_{model_name}_generated_responses_{i}.csv', index=False)\n",
    "    \n",
    "    del model, processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    categories = ['IR spectrum peak analysis', 'IR spectrum structure elucidation']\n",
    "    results = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        # Filter data for the current category\n",
    "        category_data = df[df['cls'] == category]\n",
    "        \n",
    "        # Compute accuracy\n",
    "        answer = [1 if ans == 'Yes' else 0 for ans in category_data['Answer']]\n",
    "        generated_response = [1 if 'yes' in ans.lower() else 0 if 'no' in ans.lower() else -1 for ans in category_data['Generated Response']]\n",
    "        accuracy = accuracy_score(answer, generated_response)\n",
    "        \n",
    "        # Compute F1 score\n",
    "        f1 = f1_score(answer, generated_response, average='macro')\n",
    "        \n",
    "        results[category] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "models = ['llava', 'instructBlip-7B', 'instructBlip-13B', 'Qwen-VL-Chat']\n",
    "\n",
    "for model in models:\n",
    "    for i in range(0, 3):\n",
    "        sa_f1 = []\n",
    "        sa_acc = []\n",
    "        ar_acc = []\n",
    "        ar_f1 = []\n",
    "        fg_acc = []\n",
    "        fg_f1 = []\n",
    "        matchs = []\n",
    "        print(model)\n",
    "        generated_answers = pd.read_csv(f'./data/mol_figures/step2/IR_{model}_generated_responses_{i}.csv')\n",
    "        results = evaluate_responses(generated_answers)\n",
    "        print(results)\n",
    "        sa_f1.append(results['IR spectrum peak analysis']['F1 Score'])\n",
    "        sa_acc.append(results['IR spectrum peak analysis']['Accuracy'])\n",
    "        ar_f1.append(results['IR spectrum structure elucidation']['F1 Score'])\n",
    "        ar_acc.append(results['IR spectrum structure elucidation']['Accuracy'])\n",
    "        sa_f1 = np.array(sa_f1)\n",
    "        sa_acc = np.array(sa_acc)\n",
    "        ar_acc = np.array(ar_acc)\n",
    "        ar_f1 = np.array(ar_f1)\n",
    "    print(sa_f1,sa_acc,ar_acc,ar_f1)\n",
    "    # print(sa_f1.mean(),sa_acc.mean(),ar_acc.mean(),ar_f1.mean(),fg_acc.mean(),fg_f1.mean(),matchs.mean())\n",
    "    # print(sa_f1.std(),sa_acc.std(),ar_acc.std(),ar_f1.std(),fg_acc.std(),fg_f1.std(),matchs.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COT IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "    As an expert organic chemist, your task is to analyze and determine the potential structures that can be derived from a given molecular spectrum image.\\\n",
    "    Utilize your knowledge to systematically explore and identify plausible structural configurations based on the spectrum image provided and answer the question.\\\"\\\n",
    "    please think step by step and provide the answer\"\"\"\n",
    "\n",
    "def get_gpt_response(model_name, prompt):\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert organic chemist.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    ).choices[0].message.content\n",
    "    return response\n",
    "\n",
    "iteration = 3\n",
    "data_frame = pd.DataFrame(columns=[\"question\", \"cls\", \"cot\", \"Answer\", \"Generated Response\"])\n",
    "model_names = ['instructBlip-13B', 'Qwen-VL-Chat']\n",
    "for model_name in model_names:\n",
    "    if is_open_source(model_name):\n",
    "        if 'llava' in model_name:\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                model_paths[model_name], \n",
    "                torch_dtype=torch.float16, \n",
    "                low_cpu_mem_usage=True, \n",
    "                cache_dir=cache_dir,\n",
    "                ).to(0)\n",
    "\n",
    "            processor = AutoProcessor.from_pretrained(model_paths[model_name])\n",
    "        elif 'instructBlip' in model_name:\n",
    "            model = InstructBlipForConditionalGeneration.from_pretrained(model_paths[model_name], cache_dir=cache_dir).to(\"cuda\")\n",
    "            processor = InstructBlipProcessor.from_pretrained(model_paths[model_name], cache_dir=cache_dir) \n",
    "        elif 'Qwen' in model_name:\n",
    "            processor = AutoTokenizer.from_pretrained(model_paths[model_name], trust_remote_code=True, cache_dir=cache_dir)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_paths[model_name], device_map=\"cuda\", trust_remote_code=True, cache_dir=cache_dir).eval()\n",
    "        elif 'InternVL' in model_name:\n",
    "            os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_paths[model_name],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=cache_dir,\n",
    "                device_map='auto').eval()\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, cache_dir=cache_dir)\n",
    "        else:\n",
    "            model = None\n",
    "            processor=None\n",
    "    for i in range(iteration):\n",
    "        data = pd.read_csv(f\"./data/mol_figures/step2/IR_sampled_questions_answers_{i}.csv\")\n",
    "        # data = data.sample(n=1)\n",
    "        for _, row in data.iterrows():\n",
    "            try:\n",
    "                index_parts = row['Molecule Index'].split('_')\n",
    "                index_parts = [part.strip() for part in index_parts]\n",
    "                image_path = f'./data/mol_figures/{index_parts[0]}th_specs/Problem {index_parts[1]}_1.png'\n",
    "\n",
    "                generated_cot_response = analyze_image_with_prompt(model_name, image_path, row['Question'], sys_prompt=sys_prompt, model=model, processor=processor)\n",
    "                cot_prompt = f\"Based on the following reasoning: {generated_cot_response} {row['Question']} Provide only a very short answer with the exact result. Respond with 'Yes' or 'No' to the question.\"\n",
    "                if is_open_source(model_name):\n",
    "                    generated_response = analyze_image_with_prompt(model_name, image_path, cot_prompt, 'You are an expert organic chemist.', model=model, processor=processor)\n",
    "                else:\n",
    "                    generated_response = get_gpt_response(model_name, cot_prompt)\n",
    "\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'], generated_cot_response,row['Answer'], generated_response]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    \n",
    "        data_frame.to_csv(f'./data/mol_figures/step2/IR_cot_{model_name}_generated_responses_{i}.csv', index=False)\n",
    "    del model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "model_names = ['llava', 'instructBlip-7B', 'instructBlip-13B', 'Qwen-VL-Chat']\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    for i in range(0, 3):\n",
    "        sa_f1 = []\n",
    "        sa_acc = []\n",
    "        ar_acc = []\n",
    "        ar_f1 = []\n",
    "        fg_acc = []\n",
    "        fg_f1 = []\n",
    "        matchs = []\n",
    "\n",
    "        generated_answers = pd.read_csv(f'./data/mol_figures/step2/IR_cot_{model_name}_generated_responses_{i}.csv')\n",
    "        results = evaluate_responses(generated_answers)\n",
    "        # print(results)\n",
    "        sa_f1.append(results['IR spectrum peak analysis']['F1 Score'])\n",
    "        sa_acc.append(results['IR spectrum peak analysis']['Accuracy'])\n",
    "        ar_f1.append(results['IR spectrum structure elucidation']['F1 Score'])\n",
    "        ar_acc.append(results['IR spectrum structure elucidation']['Accuracy'])\n",
    "        sa_f1 = np.array(sa_f1)\n",
    "        sa_acc = np.array(sa_acc)\n",
    "        ar_acc = np.array(ar_acc)\n",
    "        ar_f1 = np.array(ar_f1)\n",
    "    print(sa_f1.mean(),ar_f1.mean())\n",
    "    print(sa_f1.std(),ar_f1.std())\n",
    "    print(sa_acc.mean(),ar_acc.mean())\n",
    "    print(sa_acc.std(),ar_acc.std())\n",
    "    print(0.8*sa_f1.mean() + 0.2*ar_f1.mean())\n",
    "    print(0.8*sa_acc.mean() + 0.2*ar_acc.mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MASS zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "model_names = ['llava','instructBlip-7B', 'instructBlip-13B', 'Qwen-VL-Chat']\n",
    "for model_name in model_names:\n",
    "    if is_open_source(model_name):\n",
    "        if 'llava' in model_name:\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                model_paths[model_name], \n",
    "                torch_dtype=torch.float16, \n",
    "                low_cpu_mem_usage=True, \n",
    "                cache_dir=cache_dir,\n",
    "                ).to(0)\n",
    "\n",
    "            processor = AutoProcessor.from_pretrained(model_paths[model_name])\n",
    "        elif 'instructBlip' in model_name:\n",
    "            model = InstructBlipForConditionalGeneration.from_pretrained(model_paths[model_name], cache_dir=cache_dir).to(\"cuda\")\n",
    "            processor = InstructBlipProcessor.from_pretrained(model_paths[model_name], cache_dir=cache_dir) \n",
    "        elif 'Qwen' in model_name:\n",
    "            processor = AutoTokenizer.from_pretrained(model_paths[model_name], trust_remote_code=True, cache_dir=cache_dir)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_paths[model_name], device_map=\"cuda\", trust_remote_code=True, cache_dir=cache_dir).eval()\n",
    "        elif 'InternVL' in model_name:\n",
    "            os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_paths[model_name],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=cache_dir,\n",
    "                device_map='auto').eval()\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, cache_dir=cache_dir)\n",
    "        else:\n",
    "            model = None\n",
    "            processor=None\n",
    "    for i in range(0, iteration):\n",
    "        data_frame = pd.DataFrame(columns=[\"question\", \"cls\", \"Answer\", \"Generated Response\"])\n",
    "        data = pd.read_csv(f\"./data/mol_figures/step2/MASS_sampled_questions_answers_{i}.csv\")\n",
    "        for _, row in data.iterrows():\n",
    "            try:\n",
    "                index_parts = row['Molecule Index'].split('_')\n",
    "                index_parts = [part.strip() for part in index_parts]\n",
    "                # image_path = \"/home/kguo2/PycharmProjects/spectrumLM/data/mol_figures/5th_specs/Problem_21_1.png\"\n",
    "                image_path = f'./data/mol_figures/{index_parts[0]}th_specs/Problem {index_parts[1]}_2.png'\n",
    "                \n",
    "                generated_response = analyze_image_with_prompt(model_name, image_path, row['Question'], sys_prompt='You are an expert in organic chemistry. ', model=model, processor=processor)\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'], row['Answer'], generated_response]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        data_frame.to_csv(f'./data/mol_figures/step2/MASS_{model_name}_generated_responses_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    categories = ['MASS spectrum structure elucidation']\n",
    "    results = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        # Filter data for the current category\n",
    "        category_data = df[df['cls'] == category]\n",
    "        \n",
    "        # Compute accuracy\n",
    "        answer = [1 if ans == 'Yes' else 0 for ans in category_data['Answer']]\n",
    "        generated_response = [1 if 'Yes' in ans else 0 if 'No' in ans else -1 for ans in category_data['Generated Response']]\n",
    "        accuracy = accuracy_score(answer, generated_response)\n",
    "        \n",
    "        # Compute F1 score\n",
    "        f1 = f1_score(answer, generated_response, average='macro')\n",
    "        \n",
    "        results[category] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "models = ['llava','instructBlip-7B', 'instructBlip-13B', 'Qwen-VL-Chat']\n",
    "for model in models:\n",
    "    print()\n",
    "    print(model)\n",
    "\n",
    "    sa_f1 = []\n",
    "    sa_acc = []\n",
    "    ar_f1 = []\n",
    "    ar_acc = []\n",
    "    for i in range(0, 3):\n",
    "        generated_answers = pd.read_csv(f'./data/mol_figures/step2/MASS_{model}_generated_responses_{i}.csv')\n",
    "        results = evaluate_responses(generated_answers)\n",
    "        print(results)\n",
    "        ar_f1.append(results['MASS spectrum structure elucidation']['F1 Score'])\n",
    "        ar_acc.append(results['MASS spectrum structure elucidation']['Accuracy'])\n",
    "\n",
    "    ar_acc = np.array(ar_acc)\n",
    "    ar_f1 = np.array(ar_f1)\n",
    "\n",
    "    print(ar_acc.mean())\n",
    "    print(ar_acc.std())\n",
    "    print(\"f1 mean\",(ar_f1.mean()))\n",
    "    print(\"f1 std\",(ar_f1.std() ))\n",
    "    print(\"acc mean\",(ar_acc.mean())) \n",
    "    print(\"acc std\",(ar_acc.std() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cot MASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "    As an expert organic chemist, your task is to analyze and determine the potential structures that can be derived from a given molecular spectrum image.\\\n",
    "    Utilize your knowledge to systematically explore and identify plausible structural configurations based on the spectrum image provided and answer the question.\\\"\\\n",
    "    please think step by step and provide the answer\"\"\"\n",
    "\n",
    "iteration = 3\n",
    "data_frame = pd.DataFrame(columns=[\"question\", \"cls\", \"cot\", \"Answer\", \"Generated Response\"])\n",
    "model_names = ['llava', 'instructBlip-7B', 'instructBlip-13B', 'Qwen-VL-Chat']\n",
    "for model_name in model_names:\n",
    "    if is_open_source(model_name):\n",
    "        if 'llava' in model_name:\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                model_paths[model_name], \n",
    "                torch_dtype=torch.float16, \n",
    "                low_cpu_mem_usage=True, \n",
    "                cache_dir=cache_dir,\n",
    "                ).to(0)\n",
    "\n",
    "            processor = AutoProcessor.from_pretrained(model_paths[model_name])\n",
    "        elif 'instructBlip' in model_name:\n",
    "            model = InstructBlipForConditionalGeneration.from_pretrained(model_paths[model_name], cache_dir=cache_dir).to(\"cuda\")\n",
    "            processor = InstructBlipProcessor.from_pretrained(model_paths[model_name], cache_dir=cache_dir) \n",
    "        elif 'Qwen' in model_name:\n",
    "            processor = AutoTokenizer.from_pretrained(model_paths[model_name], trust_remote_code=True, cache_dir=cache_dir)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_paths[model_name], device_map=\"cuda\", trust_remote_code=True, cache_dir=cache_dir).eval()\n",
    "        elif 'InternVL' in model_name:\n",
    "            os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_paths[model_name],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=cache_dir,\n",
    "                device_map='auto').eval()\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, cache_dir=cache_dir)\n",
    "        else:\n",
    "            model = None\n",
    "            processor=None\n",
    "    for i in range(iteration):\n",
    "        data = pd.read_csv(f\"./data/mol_figures/step2/MASS_sampled_questions_answers_{i}.csv\")\n",
    "        # data = data.sample(n=1)\n",
    "        for _, row in data.iterrows():\n",
    "            try:\n",
    "                index_parts = row['Molecule Index'].split('_')\n",
    "                index_parts = [part.strip() for part in index_parts]\n",
    "                image_path = f'./data/mol_figures/{index_parts[0]}th_specs/Problem {index_parts[1]}_1.png'\n",
    "                generated_cot_response = analyze_image_with_prompt(model_name, image_path, row['Question'], sys_prompt=sys_prompt, model=model, processor=processor)\n",
    "                cot_prompt = f\"Based on the following reasoning: {generated_cot_response} {row['Question']} Provide only a very short answer with the exact result. Respond with 'Yes' or 'No' to the question.\"\n",
    "                if is_open_source(model_name):\n",
    "                    generated_response = analyze_image_with_prompt(model_name, image_path, cot_prompt, 'You are an expert organic chemist.', model=model, processor=processor)\n",
    "                else:\n",
    "                    generated_response = get_gpt_response(model_name, cot_prompt)\n",
    "\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'], generated_cot_response,row['Answer'], generated_response]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    \n",
    "        data_frame.to_csv(f'./data/mol_figures/step2/MASS_cot_{model_name}_generated_responses_{i}.csv', index=False)\n",
    "    del model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model_names = ['llava', 'instructBlip-7B', 'instructBlip-13B', 'Qwen-VL-Chat']\n",
    "for i in range(0, 3):\n",
    "    for model_name in model_names:\n",
    "        ar_acc = []\n",
    "        ar_f1 = []\n",
    "        print(model_name)\n",
    "        generated_answers = pd.read_csv(f'./data/mol_figures/step2/MASS_cot_{model_name}_generated_responses_{i}.csv')\n",
    "        results = evaluate_responses(generated_answers)\n",
    "        print(results)\n",
    "        ar_f1.append(results['MASS spectrum structure elucidation']['F1 Score'])\n",
    "        ar_acc.append(results['MASS spectrum structure elucidation']['Accuracy'])\n",
    "\n",
    "        ar_acc = np.array(ar_acc)\n",
    "        ar_f1 = np.array(ar_f1)\n",
    "    print(ar_acc,ar_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
