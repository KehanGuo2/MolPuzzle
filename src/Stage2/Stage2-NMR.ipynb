{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IR question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./data/mol_figures/step3/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MASS question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H-NMR spectrum structure elucidation': 100}\n",
      "H-NMR spectrum structure elucidation\n",
      "Sampled data saved to 'H-NMR_sampled_questions_answers.csv'\n",
      "{'H-NMR spectrum structure elucidation': 100}\n",
      "H-NMR spectrum structure elucidation\n",
      "Sampled data saved to 'H-NMR_sampled_questions_answers.csv'\n",
      "{'H-NMR spectrum structure elucidation': 100}\n",
      "H-NMR spectrum structure elucidation\n",
      "Sampled data saved to 'H-NMR_sampled_questions_answers.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "iteration = 3\n",
    "data = pd.read_csv(\"./data/mol_figures/step2/H-NMR_questions.csv\")\n",
    "ratios = {\n",
    "        'H-NMR spectrum structure elucidation': 1,\n",
    "    }\n",
    "for i in range(0, iteration):\n",
    "    total_samples = 100\n",
    "    samples_per_class = {clss: int(total_samples * ratio) for clss, ratio in ratios.items()}\n",
    "    print(samples_per_class)\n",
    "\n",
    "    sampled_data = pd.DataFrame()\n",
    "    for clss, n_samples in samples_per_class.items():\n",
    "        print(clss)\n",
    "        sampled_class_data = data[data['cls'] == clss].sample(n=n_samples)\n",
    "        sampled_data = pd.concat([sampled_data, sampled_class_data])\n",
    "\n",
    "\n",
    "    if len(sampled_data) < total_samples:\n",
    "        additional_samples = data[~data.index.isin(sampled_data.index)].sample(n=total_samples - len(sampled_data), random_state=42)\n",
    "        sampled_data = pd.concat([sampled_data, additional_samples])\n",
    "    # os.makedirs('./data/mol_figures/mol_understanding', exist_ok=True)\n",
    "    sampled_data.to_csv(f'./data/mol_figures/step2/H-NMR_sampled_questions_answers_{i}.csv', index=False)\n",
    "    print(\"Sampled data saved to 'H-NMR_sampled_questions_answers.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero-shotting learning H_NMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "import torch\n",
    "from utils import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from transformers.generation import GenerationConfig\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ['ANTHROPIC_API_KEY'] = ''\n",
    "cache_dir = \"\"\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "    As an expert organic chemist, your task is to analyze and determine the potential structures that can be derived from a given molecule's H-NMR specturm image.\\\n",
    "    Utilize your knowledge to systematically explore and identify plausible structural configurations based on these spectrum images provided and answer the question.\\\n",
    "    Identify and list possible molecular fragments that match the spectral data and Ensure the fragments are chemically feasible and consistent with the H-NMR data.\n",
    "    Analyze the problem step-by-step internally, but do not include the analysis in your output. \n",
    "    Respond with ONLY ‘Yes’ or ‘No’ to indicate whether the molecule could potentially contain the functional group\". \n",
    "    Example output: Yes.\n",
    "    \"\"\"\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def analyze_image_with_prompt_gpt(image_paths, prompt_text, sys_prompt=sys_prompt):\n",
    "    # Encode the image to base64\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    base64_images = [encode_image(image_path) for image_path in image_paths]\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": sys_prompt + \"\\n\" + prompt_text\n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "      ]\n",
    "    \n",
    "    for base64_image in base64_images:\n",
    "        messages[0]['content'].append(\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages,\n",
    "      max_tokens=500,\n",
    "      temperature=0.7\n",
    "    )\n",
    "    \n",
    "    gpt_responses = response.choices[0]\n",
    "    \n",
    "    # Return only the GPT model's responses\n",
    "    return gpt_responses\n",
    "\n",
    "def analyze_image_with_prompt_claude(image_paths, prompt_text, sys_prompt=sys_prompt):\n",
    "    \n",
    "    # Encode the image to base64\n",
    "    api_key = ''\n",
    "    client = anthropic.Anthropic(api_key=api_key)\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": 'image/png',\n",
    "                        \"data\": base64_image,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image.\"\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    )\n",
    "  \n",
    "    return message\n",
    "\n",
    "\n",
    "def analyze_image_with_prompt_llava_8B(model, processor, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    images = Image.open(image_path)\n",
    "\n",
    "    prompt = (f\"<|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n<image>\\n{prompt_text}<|eot_id|>\"\n",
    "              \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "    inputs = processor(prompt, images, return_tensors='pt').to(\"cuda\", torch.float16)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "    len_tokens = len(prompt.split())\n",
    "    generated_text = processor.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "    \n",
    "def analyze_image_with_prompt_instructBlip(model, processor, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    # if image_path is not None:\n",
    "    images = Image.open(image_path)\n",
    "    prompt = sys_prompt + prompt_text\n",
    "    inputs = processor(images=images, text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # else:\n",
    "    #     prompt = prompt_text\n",
    "    #     inputs = processor(text=sys_prompt + prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            num_beams=10,\n",
    "            max_length=512,\n",
    "            min_length=4,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.5,\n",
    "            length_penalty=1.0,\n",
    "            temperature=1,\n",
    "    )\n",
    "    generated_text = processor.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return generated_text\n",
    "\n",
    "def analyze_image_with_prompt_Qwen(model, tokenizer, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    query = tokenizer.from_list_format([\n",
    "        {'image': image_path},\n",
    "        {'text': sys_prompt+prompt_text},\n",
    "    ])\n",
    "    response, history = model.chat(tokenizer, query=query, history=None)\n",
    "    return response\n",
    "    # # 2nd dialogue turn\n",
    "    # response, history = model.chat(tokenizer, '输出\"击掌\"的检测框', history=history)\n",
    "    # print(response)\n",
    "    # # <ref>击掌</ref><box>(517,508),(589,611)</box>\n",
    "    # image = tokenizer.draw_bbox_on_latest_picture(response, history)\n",
    "    # if image:\n",
    "    #   image.save('1.jpg')\n",
    "    # else:\n",
    "    #   print(\"no box\")\n",
    "\n",
    "def analyze_image_with_prompt_InternVL(model, tokenizer, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "    IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "    \n",
    "    \n",
    "    def build_transform(input_size):\n",
    "        MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "        transform = T.Compose([\n",
    "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "        return transform\n",
    "    \n",
    "    \n",
    "    def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "        best_ratio_diff = float('inf')\n",
    "        best_ratio = (1, 1)\n",
    "        area = width * height\n",
    "        for ratio in target_ratios:\n",
    "            target_aspect_ratio = ratio[0] / ratio[1]\n",
    "            ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "            if ratio_diff < best_ratio_diff:\n",
    "                best_ratio_diff = ratio_diff\n",
    "                best_ratio = ratio\n",
    "            elif ratio_diff == best_ratio_diff:\n",
    "                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                    best_ratio = ratio\n",
    "        return best_ratio\n",
    "    \n",
    "    \n",
    "    def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "        orig_width, orig_height = image.size\n",
    "        aspect_ratio = orig_width / orig_height\n",
    "    \n",
    "        # calculate the existing image aspect ratio\n",
    "        target_ratios = set(\n",
    "            (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "            i * j <= max_num and i * j >= min_num)\n",
    "        target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "    \n",
    "        # find the closest aspect ratio to the target\n",
    "        target_aspect_ratio = find_closest_aspect_ratio(\n",
    "            aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "    \n",
    "        # calculate the target width and height\n",
    "        target_width = image_size * target_aspect_ratio[0]\n",
    "        target_height = image_size * target_aspect_ratio[1]\n",
    "        blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "    \n",
    "        # resize the image\n",
    "        resized_img = image.resize((target_width, target_height))\n",
    "        processed_images = []\n",
    "        for i in range(blocks):\n",
    "            box = (\n",
    "                (i % (target_width // image_size)) * image_size,\n",
    "                (i // (target_width // image_size)) * image_size,\n",
    "                ((i % (target_width // image_size)) + 1) * image_size,\n",
    "                ((i // (target_width // image_size)) + 1) * image_size\n",
    "            )\n",
    "            # split the image\n",
    "            split_img = resized_img.crop(box)\n",
    "            processed_images.append(split_img)\n",
    "        assert len(processed_images) == blocks\n",
    "        if use_thumbnail and len(processed_images) != 1:\n",
    "            thumbnail_img = image.resize((image_size, image_size))\n",
    "            processed_images.append(thumbnail_img)\n",
    "        return processed_images\n",
    "    \n",
    "    \n",
    "    def load_image(image_file, input_size=448, max_num=6):\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        transform = build_transform(input_size=input_size)\n",
    "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(image) for image in images]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        return pixel_values\n",
    "    \n",
    "    \n",
    "    \n",
    "    # set the max number of tiles in `max_num`\n",
    "    pixel_values = load_image(image_path, max_num=6).to(torch.bfloat16).cuda()\n",
    "    \n",
    "    generation_config = dict(\n",
    "        num_beams=1,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    # single-round single-image conversation\n",
    "    question = sys_prompt + prompt_text\n",
    "    response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "    return response\n",
    "    \n",
    "    #     # multi-round single-image conversation\n",
    "    #     question = \"请详细描述图片\" # Please describe the picture in detail\n",
    "    #     response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "    #     print(question, response)\n",
    "\n",
    "    #     question = \"请根据图片写一首诗\" # Please write a poem according to the picture\n",
    "    #     response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "    #     print(question, response)\n",
    "    \n",
    "    # # multi-round multi-image conversation\n",
    "    # pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "    # pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "    # pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "    # \n",
    "    # question = \"详细描述这两张图片\" # Describe the two pictures in detail\n",
    "    # response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "    # print(question, response)\n",
    "    # \n",
    "    # question = \"这两张图片的相同点和区别分别是什么\" # What are the similarities and differences between these two pictures\n",
    "    # response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "    # print(question, response)\n",
    "    # \n",
    "    # # batch inference (single image per sample)\n",
    "    # pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "    # pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "    # image_counts = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "    # pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "    # \n",
    "    # questions = [\"Describe the image in detail.\"] * len(image_counts)\n",
    "    # responses = model.batch_chat(tokenizer, pixel_values,\n",
    "    #                              image_counts=image_counts,\n",
    "    #                              questions=questions,\n",
    "    #                              generation_config=generation_config)\n",
    "    # for question, response in zip(questions, responses):\n",
    "    #     print(question)\n",
    "    #     print(response)\n",
    "\n",
    "\n",
    "def analyze_image_with_prompt(model_name, image_path, prompt_text, sys_prompt=sys_prompt, model=None, processor=None):\n",
    "    if 'gpt-4o' in model_name:\n",
    "        return analyze_image_with_prompt_gpt(image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif \"claude\" in model_name:\n",
    "        return analyze_image_with_prompt_claude(image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif \"llava\" in model_name:\n",
    "        return analyze_image_with_prompt_llava_8B(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif \"instructBlip\" in model_name:\n",
    "        return analyze_image_with_prompt_instructBlip(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif 'Qwen' in model_name:\n",
    "        return analyze_image_with_prompt_Qwen(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif 'InternVL' in model_name:\n",
    "        return analyze_image_with_prompt_InternVL(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose 'gpt' or 'claude'.\")\n",
    "    \n",
    "    \n",
    "def is_open_source(model_name):\n",
    "    if 'claude' in model_name or 'gemini' in model_name or 'gpt' in model_name:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "model_names = ['instructBlip-7B', 'instructBlip-13B']\n",
    "for model_name in model_names:\n",
    "    if is_open_source(model_name):\n",
    "        if 'llava' in model_name:\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                model_paths[model_name], \n",
    "                torch_dtype=torch.float16, \n",
    "                low_cpu_mem_usage=True, \n",
    "                cache_dir=cache_dir,\n",
    "                ).to(\"cuda\")\n",
    "\n",
    "            processor = AutoProcessor.from_pretrained(model_paths[model_name])\n",
    "        elif 'instructBlip' in model_name:\n",
    "            model = InstructBlipForConditionalGeneration.from_pretrained(model_paths[model_name], cache_dir=cache_dir).to(\"cuda\")\n",
    "            processor = InstructBlipProcessor.from_pretrained(model_paths[model_name], cache_dir=cache_dir) \n",
    "        elif 'Qwen' in model_name:\n",
    "            processor = AutoTokenizer.from_pretrained(model_paths[model_name], trust_remote_code=True, cache_dir=cache_dir)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_paths[model_name], device_map=\"cuda\", trust_remote_code=True, cache_dir=cache_dir).eval()\n",
    "        elif 'InternVL' in model_name:\n",
    "            os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_paths[model_name],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=cache_dir,\n",
    "                device_map='auto').eval()\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, cache_dir=cache_dir)\n",
    "        else:\n",
    "            model = None\n",
    "            processor=None\n",
    "            \n",
    "    for i in range(0, iteration):\n",
    "        data_frame = pd.DataFrame(columns=[\"question\", \"cls\", \"Answer\", \"Generated Response\"])\n",
    "        data = pd.read_csv(f\"./data/mol_figures/step2/H-NMR_sampled_questions_answers_{i}.csv\")\n",
    "        for _, row in data.iterrows():\n",
    "            try:\n",
    "                index_parts = row['Molecule Index'].split('_')\n",
    "                index_parts = [part.strip() for part in index_parts]\n",
    "                # image_path = \"/home/kguo2/PycharmProjects/spectrumLM/data/mol_figures/5th_specs/Problem_21_1.png\"\n",
    "                image_path = f'./data/mol_figures/{index_parts[0]}th_specs/Problem {index_parts[1]}_4.png'\n",
    "\n",
    "                generated_response = analyze_image_with_prompt(model_name, image_path, row['Question'], sys_prompt=sys_prompt, model=model, processor=processor)\n",
    "                print(generated_response)\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'], row['Answer'], generated_response]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        data_frame.to_csv(f'./data/mol_figures/step2/H-NMR_{model_name}_generated_responses_{i}.csv', index=False)\n",
    "    \n",
    "    del model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    categories = ['H-NMR spectrum structure elucidation']\n",
    "    results = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        # Filter data for the current category\n",
    "        category_data = df[df['cls'] == category]\n",
    "        \n",
    "        # Compute accuracy\n",
    "        answer = [1 if ans == 'Yes' else 0 for ans in category_data['Answer']]\n",
    "        generated_response = [1 if 'yes' in ans.lower() else 0 if 'no' in ans.lower() else -1 for ans in category_data['Generated Response']]\n",
    "        accuracy = accuracy_score(answer, generated_response)\n",
    "        \n",
    "        # Compute F1 score\n",
    "        f1 = f1_score(answer, generated_response, average='macro')\n",
    "        \n",
    "        results[category] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "models = ['instructBlip-7B', 'instructBlip-13B']\n",
    "\n",
    "for model in models:\n",
    "    print()\n",
    "    print(model)\n",
    "\n",
    "    sa_f1 = []\n",
    "    sa_acc = []\n",
    "    ar_f1 = []\n",
    "    ar_acc = []\n",
    "    for i in range(0, 3):\n",
    "        generated_answers = pd.read_csv(f'./data/mol_figures/step2/H-NMR_{model}_generated_responses_{i}.csv')\n",
    "        results = evaluate_responses(generated_answers)\n",
    "        # print(results)\n",
    "\n",
    "        ar_f1.append(results['H-NMR spectrum structure elucidation']['F1 Score'])\n",
    "        ar_acc.append(results['H-NMR spectrum structure elucidation']['Accuracy'])\n",
    "    ar_acc = np.array(ar_acc)\n",
    "    ar_f1 = np.array(ar_f1)\n",
    "    print(ar_f1.mean())\n",
    "    print(ar_f1.std())\n",
    "    print(ar_acc.mean())\n",
    "    print(ar_acc.std())\n",
    "    print(\"f1 mean\", ar_f1.mean())\n",
    "    print(\"f1 std\", ar_f1.std())\n",
    "    print(\"acc mean\", ar_acc.mean())\n",
    "    print(\"acc std\", ar_acc.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C_NMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "import torch\n",
    "from utils import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from transformers.generation import GenerationConfig\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "    As an expert organic chemist, your task is to analyze and determine the potential structures that can be derived from a given molecule's C-NMR specturm image.\\\n",
    "    Utilize your knowledge to systematically explore and identify plausible structural configurations based on these spectrum images provided and answer the question.\\\n",
    "    Identify and list possible molecular fragments that match the spectral data and Ensure the fragments are chemically feasible and consistent with the C-NMR data.\n",
    "    Analyze the problem step-by-step internally, but do not include the analysis in your output. \n",
    "    Respond with ONLY ‘Yes’ or ‘No’ to indicate whether the molecule could potentially contain the functional group\". Example output:Yes.\n",
    "    \"\"\"\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def analyze_image_with_prompt_gpt(image_paths, prompt_text, sys_prompt=sys_prompt):\n",
    "    # Encode the image to base64\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    base64_images = [encode_image(image_path) for image_path in image_paths]\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": sys_prompt + \"\\n\" + prompt_text\n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "      ]\n",
    "    \n",
    "    for base64_image in base64_images:\n",
    "        messages[0]['content'].append(\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages,\n",
    "      max_tokens=500,\n",
    "      temperature=0.7\n",
    "    )\n",
    "    \n",
    "    gpt_responses = response.choices[0]\n",
    "    \n",
    "    # Return only the GPT model's responses\n",
    "    return gpt_responses\n",
    "\n",
    "def analyze_image_with_prompt_claude(image_paths, prompt_text, sys_prompt=sys_prompt):\n",
    "    \n",
    "    # Encode the image to base64\n",
    "    api_key = ''\n",
    "    client = anthropic.Anthropic(api_key=api_key)\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": 'image/png',\n",
    "                        \"data\": base64_image,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image.\"\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    )\n",
    "  \n",
    "    return message\n",
    "\n",
    "\n",
    "def analyze_image_with_prompt_llava_8B(model, processor, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    images = Image.open(image_path)\n",
    "\n",
    "    prompt = (f\"<|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n<image>\\n{prompt_text}<|eot_id|>\"\n",
    "              \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "    inputs = processor(prompt, images, return_tensors='pt').to(\"cuda\", torch.float16)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128, do_sample=True)\n",
    "    len_tokens = len(prompt.split())\n",
    "    generated_text = processor.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "    \n",
    "def analyze_image_with_prompt_instructBlip(model, processor, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    # if image_path is not None:\n",
    "    images = Image.open(image_path)\n",
    "    prompt = sys_prompt + prompt_text\n",
    "    inputs = processor(images=images, text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # else:\n",
    "    #     prompt = prompt_text\n",
    "    #     inputs = processor(text=sys_prompt + prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            num_beams=5,\n",
    "            max_length=512,\n",
    "            min_length=4,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.5,\n",
    "            length_penalty=1.0,\n",
    "            temperature=1,\n",
    "    )\n",
    "    generated_text = processor.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return generated_text\n",
    "\n",
    "def analyze_image_with_prompt_Qwen(model, tokenizer, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    query = tokenizer.from_list_format([\n",
    "        {'image': image_path},\n",
    "        {'text': sys_prompt+prompt_text},\n",
    "    ])\n",
    "    response, history = model.chat(tokenizer, query=query, history=None)\n",
    "    return response\n",
    "    # # 2nd dialogue turn\n",
    "    # response, history = model.chat(tokenizer, '输出\"击掌\"的检测框', history=history)\n",
    "    # print(response)\n",
    "    # # <ref>击掌</ref><box>(517,508),(589,611)</box>\n",
    "    # image = tokenizer.draw_bbox_on_latest_picture(response, history)\n",
    "    # if image:\n",
    "    #   image.save('1.jpg')\n",
    "    # else:\n",
    "    #   print(\"no box\")\n",
    "\n",
    "def analyze_image_with_prompt_InternVL(model, tokenizer, image_path, prompt_text, sys_prompt=sys_prompt):\n",
    "    IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "    IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "    \n",
    "    \n",
    "    def build_transform(input_size):\n",
    "        MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "        transform = T.Compose([\n",
    "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "        return transform\n",
    "    \n",
    "    \n",
    "    def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "        best_ratio_diff = float('inf')\n",
    "        best_ratio = (1, 1)\n",
    "        area = width * height\n",
    "        for ratio in target_ratios:\n",
    "            target_aspect_ratio = ratio[0] / ratio[1]\n",
    "            ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "            if ratio_diff < best_ratio_diff:\n",
    "                best_ratio_diff = ratio_diff\n",
    "                best_ratio = ratio\n",
    "            elif ratio_diff == best_ratio_diff:\n",
    "                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                    best_ratio = ratio\n",
    "        return best_ratio\n",
    "    \n",
    "    \n",
    "    def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "        orig_width, orig_height = image.size\n",
    "        aspect_ratio = orig_width / orig_height\n",
    "    \n",
    "        # calculate the existing image aspect ratio\n",
    "        target_ratios = set(\n",
    "            (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "            i * j <= max_num and i * j >= min_num)\n",
    "        target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "    \n",
    "        # find the closest aspect ratio to the target\n",
    "        target_aspect_ratio = find_closest_aspect_ratio(\n",
    "            aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "    \n",
    "        # calculate the target width and height\n",
    "        target_width = image_size * target_aspect_ratio[0]\n",
    "        target_height = image_size * target_aspect_ratio[1]\n",
    "        blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "    \n",
    "        # resize the image\n",
    "        resized_img = image.resize((target_width, target_height))\n",
    "        processed_images = []\n",
    "        for i in range(blocks):\n",
    "            box = (\n",
    "                (i % (target_width // image_size)) * image_size,\n",
    "                (i // (target_width // image_size)) * image_size,\n",
    "                ((i % (target_width // image_size)) + 1) * image_size,\n",
    "                ((i // (target_width // image_size)) + 1) * image_size\n",
    "            )\n",
    "            # split the image\n",
    "            split_img = resized_img.crop(box)\n",
    "            processed_images.append(split_img)\n",
    "        assert len(processed_images) == blocks\n",
    "        if use_thumbnail and len(processed_images) != 1:\n",
    "            thumbnail_img = image.resize((image_size, image_size))\n",
    "            processed_images.append(thumbnail_img)\n",
    "        return processed_images\n",
    "    \n",
    "    \n",
    "    def load_image(image_file, input_size=448, max_num=6):\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        transform = build_transform(input_size=input_size)\n",
    "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(image) for image in images]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        return pixel_values\n",
    "    \n",
    "    \n",
    "    \n",
    "    # set the max number of tiles in `max_num`\n",
    "    pixel_values = load_image(image_path, max_num=6).to(torch.bfloat16).cuda()\n",
    "    \n",
    "    generation_config = dict(\n",
    "        num_beams=1,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    # single-round single-image conversation\n",
    "    question = sys_prompt + prompt_text\n",
    "    response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "    return response\n",
    "    \n",
    "    #     # multi-round single-image conversation\n",
    "    #     question = \"请详细描述图片\" # Please describe the picture in detail\n",
    "    #     response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "    #     print(question, response)\n",
    "\n",
    "    #     question = \"请根据图片写一首诗\" # Please write a poem according to the picture\n",
    "    #     response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "    #     print(question, response)\n",
    "    \n",
    "    # # multi-round multi-image conversation\n",
    "    # pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "    # pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "    # pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "    # \n",
    "    # question = \"详细描述这两张图片\" # Describe the two pictures in detail\n",
    "    # response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "    # print(question, response)\n",
    "    # \n",
    "    # question = \"这两张图片的相同点和区别分别是什么\" # What are the similarities and differences between these two pictures\n",
    "    # response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "    # print(question, response)\n",
    "    # \n",
    "    # # batch inference (single image per sample)\n",
    "    # pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "    # pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "    # image_counts = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "    # pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "    # \n",
    "    # questions = [\"Describe the image in detail.\"] * len(image_counts)\n",
    "    # responses = model.batch_chat(tokenizer, pixel_values,\n",
    "    #                              image_counts=image_counts,\n",
    "    #                              questions=questions,\n",
    "    #                              generation_config=generation_config)\n",
    "    # for question, response in zip(questions, responses):\n",
    "    #     print(question)\n",
    "    #     print(response)\n",
    "\n",
    "\n",
    "def analyze_image_with_prompt(model_name, image_path, prompt_text, sys_prompt=sys_prompt, model=None, processor=None):\n",
    "    if 'gpt-4o' in model_name:\n",
    "        return analyze_image_with_prompt_gpt(image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif \"claude\" in model_name:\n",
    "        return analyze_image_with_prompt_claude(image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif \"llava\" in model_name:\n",
    "        return analyze_image_with_prompt_llava_8B(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif \"instructBlip\" in model_name:\n",
    "        return analyze_image_with_prompt_instructBlip(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif 'Qwen' in model_name:\n",
    "        return analyze_image_with_prompt_Qwen(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    elif 'InternVL' in model_name:\n",
    "        return analyze_image_with_prompt_InternVL(model, processor, image_path, prompt_text, sys_prompt=sys_prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose 'gpt' or 'claude'.\")\n",
    "    \n",
    "    \n",
    "def is_open_source(model_name):\n",
    "    if 'claude' in model_name or 'gemini' in model_name or 'gpt' in model_name:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "model_names = ['llava', 'instructBlip-7B', 'instructBlip-13B', 'Qwen-VL-Chat']\n",
    "for model_name in model_names:\n",
    "    if is_open_source(model_name):\n",
    "        if 'llava' in model_name:\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                model_paths[model_name], \n",
    "                torch_dtype=torch.float16, \n",
    "                low_cpu_mem_usage=True, \n",
    "                cache_dir=cache_dir,\n",
    "                ).to(\"cuda\")\n",
    "\n",
    "            processor = AutoProcessor.from_pretrained(model_paths[model_name])\n",
    "        elif 'instructBlip' in model_name:\n",
    "            model = InstructBlipForConditionalGeneration.from_pretrained(model_paths[model_name], cache_dir=cache_dir).to(\"cuda\")\n",
    "            processor = InstructBlipProcessor.from_pretrained(model_paths[model_name], cache_dir=cache_dir) \n",
    "        elif 'Qwen' in model_name:\n",
    "            processor = AutoTokenizer.from_pretrained(model_paths[model_name], trust_remote_code=True, cache_dir=cache_dir)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_paths[model_name], device_map=\"cuda\", trust_remote_code=True, cache_dir=cache_dir).eval()\n",
    "        elif 'InternVL' in model_name:\n",
    "            os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_paths[model_name],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=cache_dir,\n",
    "                device_map='auto').eval()\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, cache_dir=cache_dir)\n",
    "        else:\n",
    "            model = None\n",
    "            processor=None\n",
    "            \n",
    "    for i in range(0, iteration):\n",
    "        data_frame = pd.DataFrame(columns=[\"question\", \"cls\", \"Answer\", \"Generated Response\"])\n",
    "        data = pd.read_csv(f\"./data/mol_figures/step2/C-NMR_sampled_questions_answers_{i}.csv\")\n",
    "        for _, row in data.iterrows():\n",
    "            try:\n",
    "                index_parts = row['Molecule Index'].split('_')\n",
    "                index_parts = [part.strip() for part in index_parts]\n",
    "                # image_path = \"/home/kguo2/PycharmProjects/spectrumLM/data/mol_figures/5th_specs/Problem_21_1.png\"\n",
    "                image_path = f'./data/mol_figures/{index_parts[0]}th_specs/Problem {index_parts[1]}_3.png'\n",
    "\n",
    "                generated_response = analyze_image_with_prompt(model_name, image_path, row['Question'], sys_prompt=sys_prompt, model=model, processor=processor)\n",
    "                print(generated_response)\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'], row['Answer'], generated_response]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        data_frame.to_csv(f'./data/mol_figures/step2/C-NMR_{model_name}_generated_responses_{i}.csv', index=False)\n",
    "    \n",
    "    del model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    categories = ['C-NMR spectrum structure elucidation']\n",
    "    results = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        # Filter data for the current category\n",
    "        category_data = df[df['cls'] == category]\n",
    "        \n",
    "        # Compute accuracy\n",
    "        answer = [1 if ans == 'Yes' else 0 for ans in category_data['Answer']]\n",
    "        generated_response = [1 if 'yes' in ans else 0 if 'no' in ans else -1 for ans in category_data['Generated Response'].apply(str)]\n",
    "        accuracy = accuracy_score(answer, generated_response)\n",
    "        \n",
    "        # Compute F1 score\n",
    "        f1 = f1_score(answer, generated_response, average='macro')\n",
    "        \n",
    "        results[category] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "models = [ 'instructBlip-7B', 'instructBlip-13B', 'Qwen-VL-Chat']\n",
    "\n",
    "for model in models:\n",
    "    print()\n",
    "    print(model)\n",
    "\n",
    "    sa_f1 = []\n",
    "    sa_acc = []\n",
    "    ar_f1 = []\n",
    "    ar_acc = []\n",
    "    for i in range(0, 3):\n",
    "        generated_answers = pd.read_csv(f'./data/mol_figures/step2/C-NMR_{model}_generated_responses_{i}.csv')\n",
    "        results = evaluate_responses(generated_answers)\n",
    "        # print(results)\n",
    "\n",
    "        ar_f1.append(results['C-NMR spectrum structure elucidation']['F1 Score'])\n",
    "        ar_acc.append(results['C-NMR spectrum structure elucidation']['Accuracy'])\n",
    "    ar_acc = np.array(ar_acc)\n",
    "    ar_f1 = np.array(ar_f1)\n",
    "    print(ar_f1.mean())\n",
    "    print(ar_f1.std())\n",
    "    print(ar_acc.mean())\n",
    "    print(ar_acc.std())\n",
    "    print(\"f1 mean\", ar_f1.mean())\n",
    "    print(\"f1 std\", ar_f1.std())\n",
    "    print(\"acc mean\", ar_acc.mean())\n",
    "    print(\"acc std\", ar_acc.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
