{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fcd\n",
      "  Downloading fcd-1.2.2-py3-none-any.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/kguo2/anaconda3/envs/pygeo/lib/python3.9/site-packages (from fcd) (1.8.1)\n",
      "Requirement already satisfied: numpy in /home/kguo2/anaconda3/envs/pygeo/lib/python3.9/site-packages (from fcd) (1.24.2)\n",
      "Requirement already satisfied: torch in /home/kguo2/anaconda3/envs/pygeo/lib/python3.9/site-packages (from fcd) (1.13.0+cu116)\n",
      "Requirement already satisfied: rdkit in /home/kguo2/anaconda3/envs/pygeo/lib/python3.9/site-packages (from fcd) (2022.9.5)\n",
      "Requirement already satisfied: Pillow in /home/kguo2/anaconda3/envs/pygeo/lib/python3.9/site-packages (from rdkit->fcd) (9.4.0)\n",
      "Requirement already satisfied: typing-extensions in /home/kguo2/anaconda3/envs/pygeo/lib/python3.9/site-packages (from torch->fcd) (4.11.0)\n",
      "Installing collected packages: fcd\n",
      "Successfully installed fcd-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from Levenshtein import distance as lev\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from fcd import get_fcd, load_ref_model, canonical_smiles\n",
    "\n",
    "from rdkit import Chem\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from Levenshtein import distance as lev\n",
    "import numpy as np\n",
    "\n",
    "# def evaluate1(outputs, verbose=False):\n",
    "#     if not outputs:\n",
    "#         raise ValueError(\"Output list is empty\")\n",
    "\n",
    "#     references, hypotheses = [], []\n",
    "#     bleu_scores, levs, bad_mols, num_exact = [], [], 0, 0\n",
    "\n",
    "#     for i, (_, gt, out) in enumerate(outputs):\n",
    "#         if verbose and i % 100 == 0:\n",
    "#             print(f'{i} processed.')\n",
    "\n",
    "#         # Tokenization for BLEU score\n",
    "#         references.append([list(gt)])\n",
    "#         hypotheses.append(list(out))\n",
    "\n",
    "#         # Handling chemical structure comparison\n",
    "#         try:\n",
    "#             m_out = Chem.MolFromSmiles(out)\n",
    "#             m_gt = Chem.MolFromSmiles(gt)\n",
    "#             if Chem.MolToInchi(m_out) == Chem.MolToInchi(m_gt):\n",
    "#                 num_exact += 1\n",
    "#         except Exception as e:\n",
    "#             if verbose:\n",
    "#                 print(f\"Error processing molecules: {e}\")\n",
    "#             bad_mols += 1\n",
    "\n",
    "#         # Levenshtein distance\n",
    "#         levs.append(lev(out, gt))\n",
    "\n",
    "#     # Calculate BLEU score\n",
    "#     # bleu_score = corpus_bleu(references, hypotheses)\n",
    "#     # if verbose:\n",
    "#     #     print('BLEU score:', bleu_score)\n",
    "\n",
    "#     # Exact matching score\n",
    "#     exact_match_score = num_exact / len(outputs)\n",
    "#     levenshtein_score = np.mean(levs)\n",
    "#     validity_score = 1 - bad_mols / len(outputs)\n",
    "\n",
    "#     if verbose:\n",
    "#         print('Exact Match:', exact_match_score)\n",
    "#         print('Levenshtein:', levenshtein_score)\n",
    "#         print('Validity:', validity_score)\n",
    "\n",
    "#     return  exact_match_score, levenshtein_score, validity_score\n",
    "\n",
    "\n",
    "def evaluate1(outputs, verbose=False):\n",
    "#     outputs = []\n",
    "\n",
    "#     with open(osp.join(input_fp)) as f:\n",
    "#         reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "#         for n, line in enumerate(reader):\n",
    "#             gt_smi = line['ground truth']\n",
    "#             ot_smi = line['output']\n",
    "#             outputs.append((line['description'], gt_smi, ot_smi))\n",
    "\n",
    "\n",
    "    bleu_scores = []\n",
    "    #meteor_scores = []\n",
    "\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    for i, (_, gt, out) in enumerate(outputs):\n",
    "        print(gt,out)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            if verbose:\n",
    "                print(i, 'processed.')\n",
    "\n",
    "\n",
    "        gt_tokens = [c for c in gt]\n",
    "\n",
    "        out_tokens = [c for c in out]\n",
    "\n",
    "        references.append([gt_tokens])\n",
    "        hypotheses.append([out_tokens])\n",
    "\n",
    "        # mscore = meteor_score([gt], out)\n",
    "        # meteor_scores.append(mscore)\n",
    "\n",
    "    # BLEU score\n",
    "    # bleu_score = corpus_bleu(references, hypotheses)\n",
    "    # if verbose: print('BLEU score:', bleu_score)\n",
    "\n",
    "    # Meteor score\n",
    "    # _meteor_score = np.mean(meteor_scores)\n",
    "    # print('Average Meteor score:', _meteor_score)\n",
    "\n",
    "    rouge_scores = []\n",
    "\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    levs = []\n",
    "\n",
    "    num_exact = 0\n",
    "\n",
    "    bad_mols = 0\n",
    "\n",
    "    for i, (_, gt, out) in enumerate(outputs):\n",
    "\n",
    "        hypotheses.append(out)\n",
    "        references.append(gt)\n",
    "\n",
    "        try:\n",
    "            m_out = Chem.MolFromSmiles(out)\n",
    "            m_gt = Chem.MolFromSmiles(gt)\n",
    "\n",
    "            if Chem.MolToInchi(m_out) == Chem.MolToInchi(m_gt): num_exact += 1\n",
    "            #if gt == out: num_exact += 1 #old version that didn't standardize strings\n",
    "        except:\n",
    "            bad_mols += 1\n",
    "\n",
    "        \n",
    "\n",
    "        levs.append(lev(out, gt))\n",
    "\n",
    "\n",
    "    # Exact matching score\n",
    "    exact_match_score = num_exact/(i+1)\n",
    "    if verbose:\n",
    "        print('Exact Match:')\n",
    "        print(exact_match_score)\n",
    "\n",
    "    # Levenshtein score\n",
    "    levenshtein_score = np.mean(levs)\n",
    "    if verbose:\n",
    "        print('Levenshtein:')\n",
    "        print(levenshtein_score)\n",
    "        \n",
    "    validity_score = 1 - bad_mols/len(out)\n",
    "    if verbose:\n",
    "        print('validity:', validity_score)\n",
    "\n",
    "    return  exact_match_score, levenshtein_score, validity_score\n",
    "\n",
    "\n",
    "\n",
    "def evaluate2(raw_outputs, morgan_r=2, verbose=False):\n",
    "    bad_mols = 0\n",
    "    outputs = []\n",
    "\n",
    "    for _, gt_smi, ot_smi in raw_outputs:\n",
    "        try:\n",
    "            gt_m = Chem.MolFromSmiles(gt_smi)\n",
    "            ot_m = Chem.MolFromSmiles(ot_smi)\n",
    "\n",
    "            if ot_m == None: raise ValueError('Bad SMILES')\n",
    "            outputs.append((_, gt_m, ot_m))\n",
    "        except:\n",
    "            bad_mols += 1\n",
    "    validity_score = len(outputs)/(len(outputs)+bad_mols)\n",
    "    if verbose:\n",
    "        print('validity:', validity_score)\n",
    "\n",
    "\n",
    "    MACCS_sims = []\n",
    "    morgan_sims = []\n",
    "    RDK_sims = []\n",
    "\n",
    "    enum_list = outputs\n",
    "\n",
    "    for i, (_, gt_m, ot_m) in enumerate(enum_list):\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            if verbose: print(i, 'processed.')\n",
    "\n",
    "        MACCS_sims.append(DataStructs.FingerprintSimilarity(MACCSkeys.GenMACCSKeys(gt_m), MACCSkeys.GenMACCSKeys(ot_m), metric=DataStructs.TanimotoSimilarity))\n",
    "        RDK_sims.append(DataStructs.FingerprintSimilarity(Chem.RDKFingerprint(gt_m), Chem.RDKFingerprint(ot_m), metric=DataStructs.TanimotoSimilarity))\n",
    "        morgan_sims.append(DataStructs.TanimotoSimilarity(AllChem.GetMorganFingerprint(gt_m,morgan_r), AllChem.GetMorganFingerprint(ot_m, morgan_r)))\n",
    "\n",
    "    maccs_sims_score = np.mean(MACCS_sims)\n",
    "    rdk_sims_score = np.mean(RDK_sims)\n",
    "    morgan_sims_score = np.mean(morgan_sims)\n",
    "    if verbose:\n",
    "        print('Average MACCS Similarity:', maccs_sims_score)\n",
    "        print('Average RDK Similarity:', rdk_sims_score)\n",
    "        print('Average Morgan Similarity:', morgan_sims_score)\n",
    "    return validity_score, maccs_sims_score, rdk_sims_score, morgan_sims_score\n",
    "\n",
    "\n",
    "def evaluate3(gt_smis, ot_smis, verbose=False):\n",
    "#     gt_smis = []\n",
    "#     ot_smis = []\n",
    "\n",
    "#     with open(osp.join(input_file)) as f:\n",
    "#         reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "#         for n, line in enumerate(reader):\n",
    "#             gt_smi = line['ground truth']\n",
    "#             ot_smi = line['output']\n",
    "#             if len(ot_smi) == 0: ot_smi = '[]'\n",
    "\n",
    "#             gt_smis.append(gt_smi)\n",
    "#             ot_smis.append(ot_smi)\n",
    "\n",
    "\n",
    "    model = load_ref_model()\n",
    "\n",
    "    canon_gt_smis = [w for w in canonical_smiles(gt_smis) if w is not None]\n",
    "    canon_ot_smis = [w for w in canonical_smiles(ot_smis) if w is not None]\n",
    "\n",
    "    fcd_sim_score = get_fcd(canon_gt_smis, canon_ot_smis, model)\n",
    "    if verbose:\n",
    "        print('FCD Similarity:', fcd_sim_score)\n",
    "\n",
    "    return fcd_sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "exact_match_scores = []\n",
    "levenshtein_scores = []\n",
    "validity_scores = []\n",
    "maccs_sims_scores = []\n",
    "rdk_sims_scores = []\n",
    "morgan_sims_scores= []\n",
    "fcd_scores = []\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from Levenshtein import distance as lev_distance\n",
    "from fcd import get_fcd, load_ref_model, canonical_smiles\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model for FCD calculations\n",
    "model = load_ref_model()\n",
    "\n",
    "def calculate_scores(smiles1, smiles2):\n",
    "    # Convert SMILES to RDKit molecule objects\n",
    "    mol1 = Chem.MolFromSmiles(smiles1)\n",
    "    mol2 = Chem.MolFromSmiles(smiles2)\n",
    "    \n",
    "    # Validity checks\n",
    "    validity1 = 1 if mol1 else 0\n",
    "    validity2 = 1 if mol2 else 0\n",
    "    validity = validity1 * validity2  # Both must be valid\n",
    "    \n",
    "    # Exact match\n",
    "    exact_match = 1 if smiles1 == smiles2 else 0\n",
    "    \n",
    "    # Levenshtein distance\n",
    "    levenshtein_dist = lev_distance(smiles1, smiles2)\n",
    "    \n",
    "    # MACCS Fingerprint Similarity\n",
    "    fps1 = MACCSkeys.GenMACCSKeys(mol1)\n",
    "    fps2 = MACCSkeys.GenMACCSKeys(mol2)\n",
    "    maccs_sim = DataStructs.FingerprintSimilarity(fps1, fps2)\n",
    "    \n",
    "    # RDKit Fingerprint Similarity\n",
    "    rdk_fps1 = Chem.RDKFingerprint(mol1)\n",
    "    rdk_fps2 = Chem.RDKFingerprint(mol2)\n",
    "    rdk_sim = DataStructs.FingerprintSimilarity(rdk_fps1, rdk_fps2)\n",
    "    \n",
    "    # Morgan Fingerprint Similarity\n",
    "    morgan_fps1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=2048)\n",
    "    morgan_fps2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=2048)\n",
    "    morgan_sim = DataStructs.FingerprintSimilarity(morgan_fps1, morgan_fps2)\n",
    "\n",
    "    # FCD Score\n",
    "    # Canonicalize and filter None values\n",
    "    canon_gt_smis = [w for w in canonical_smiles([smiles2]) if w is not None]\n",
    "    canon_ot_smis = [w for w in canonical_smiles([smiles1]) if w is not None]\n",
    "    # fcd_score = get_fcd(canon_gt_smis, canon_ot_smis, model) if canon_gt_smis and canon_ot_smis else np.nan\n",
    "\n",
    "    return {\n",
    "        \"Exact Match\": exact_match,\n",
    "        \"Levenshtein Distance\": levenshtein_dist,\n",
    "        \"Validity\": validity,\n",
    "        \"MACCS FTS\": maccs_sim,\n",
    "        \"RDK FTS\": rdk_sim,\n",
    "        \"Morgan FTS\": morgan_sim,\n",
    "        # \"FCD\": fcd_score\n",
    "    }\n",
    "iteration  = 3\n",
    "model = ['gpt','gemi','cloude']\n",
    "\n",
    "for model_name in model:\n",
    "    ex_score = []\n",
    "    le_score = []\n",
    "    va_score = []\n",
    "    ma_score = []\n",
    "    rd_score = []\n",
    "    mo_score = []\n",
    "\n",
    "    for i in range(1,iteration+1):\n",
    "        result = pd.read_csv(f'./results_{model_name}_{i}.csv')\n",
    "        exact_match_scores  = 0\n",
    "        levenshtein_scores  = []\n",
    "        validity_scores  = 0\n",
    "        maccs_sims_scores  =  []\n",
    "        rdk_sims_scores  = []\n",
    "        morgan_sims_scores = []\n",
    "\n",
    "        for index, row in result.iterrows():\n",
    "            output, gt = row['selected molecule'], row['SMILES']\n",
    "            scores = calculate_scores(gt, output)\n",
    "            exact_match_scores += scores[\"Exact Match\"]\n",
    "            levenshtein_scores.append(scores[\"Levenshtein Distance\"])\n",
    "            validity_scores += scores[\"Validity\"]\n",
    "            maccs_sims_scores.append(scores[\"MACCS FTS\"])\n",
    "            rdk_sims_scores.append(scores[\"RDK FTS\"])\n",
    "            morgan_sims_scores.append(scores[\"Morgan FTS\"])\n",
    "\n",
    "        exact_match_scores = exact_match_scores / len(result)\n",
    "        validity_scores = validity_scores / len(result)\n",
    "        levenshtein_scores = np.mean(levenshtein_scores)\n",
    "        maccs_sims_scores = np.mean(maccs_sims_scores)\n",
    "        rdk_sims_scores = np.mean(rdk_sims_scores)\n",
    "        morgan_sims_scores = np.mean(morgan_sims_scores)\n",
    "        ex_score.append(exact_match_scores)\n",
    "        le_score.append(levenshtein_scores)\n",
    "        va_score.append(validity_scores)\n",
    "        ma_score.append(maccs_sims_scores)\n",
    "        rd_score.append(rdk_sims_scores)\n",
    "        mo_score.append(morgan_sims_scores)\n",
    "    print(model_name)\n",
    "    print(np.mean(ex_score),np.mean(le_score),np.mean(va_score),np.mean(ma_score),np.mean(rd_score),np.mean(mo_score))\n",
    "    print(np.std(ex_score),np.std(le_score),np.std(va_score),np.std(ma_score),np.std(rd_score),np.std(mo_score))\n",
    "    print('---------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exact Match:', exact_match_scores)\n",
    "print('Validity:', validity_scores)\n",
    "print('Levenshtein:', levenshtein_scores)\n",
    "print('MACCS:', maccs_sims_scores)\n",
    "print('RDK:', rdk_sims_scores)\n",
    "print('Morgan:', morgan_sims_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_molecule_design(details_df):\n",
    "    bleu_scores = []\n",
    "    exact_match_scores = []\n",
    "    levenshtein_scores = []\n",
    "    validity_scores = []\n",
    "    maccs_sims_scores = []\n",
    "    rdk_sims_scores = []\n",
    "    morgan_sims_scores = []\n",
    "    fcd_scores = []\n",
    "\n",
    "    fcds = []\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        tem = list(zip(details_df['description'], details_df['SMILES'], details_df['pred_{}'.format(i)]))\n",
    "#         tem2 = list(zip(details_df['SMILES'], details_df['pred_{}'.format(i)]))\n",
    "        \n",
    "        bleu_score, exact_match_score, levenshtein_score, _ = evaluate1(tem)\n",
    "        validity_score, maccs_sims_score, rdk_sims_score, morgan_sims_score = evaluate2(tem)\n",
    "        fcd = evaluate3(list(details_df['SMILES']), list(details_df['pred_{}'.format(i)]))\n",
    "        \n",
    "        bleu_scores.append(bleu_score)\n",
    "        exact_match_scores.append(exact_match_score)\n",
    "        levenshtein_scores.append(levenshtein_score)\n",
    "        \n",
    "        validity_scores.append(validity_score)\n",
    "        maccs_sims_scores.append(maccs_sims_score)\n",
    "        rdk_sims_scores.append(rdk_sims_score)\n",
    "        morgan_sims_scores.append(morgan_sims_score)\n",
    "        \n",
    "        fcd_scores.append(fcd)\n",
    "    \n",
    "    bleu_mean = np.mean(bleu_scores)\n",
    "    exact_match_mean = np.mean(exact_match_scores)\n",
    "    levenshtein_mean = np.mean(levenshtein_scores)\n",
    "    validity_mean = np.mean(validity_scores)\n",
    "    maccs_sims_mean = np.mean(maccs_sims_scores)\n",
    "    rdk_sims_mean = np.mean(rdk_sims_scores)\n",
    "    morgan_sims_mean = np.mean(morgan_sims_scores)\n",
    "    fcd_mean = np.mean(fcd_scores)\n",
    "    \n",
    "\n",
    "    # cal std\n",
    "    bleu_variance = statistics.stdev(bleu_scores)\n",
    "    exact_match_variance = statistics.stdev(exact_match_scores) \n",
    "    levenshtein_variance = statistics.stdev(levenshtein_scores)\n",
    "    validity_variance = statistics.stdev(validity_scores)\n",
    "    maccs_sims_variance = statistics.stdev(maccs_sims_scores) \n",
    "    rdk_sims_variance = statistics.stdev(rdk_sims_scores) \n",
    "    morgan_sims_variance = statistics.stdev(morgan_sims_scores) \n",
    "    fcd_variance = statistics.stdev(fcd_scores) \n",
    "    \n",
    "    stds = [bleu_variance, exact_match_variance, levenshtein_variance, \n",
    "             validity_variance, maccs_sims_variance, rdk_sims_variance, \n",
    "             morgan_sims_variance, fcd_variance]\n",
    "    \n",
    "    return [bleu_mean, exact_match_mean, levenshtein_mean, validity_mean, maccs_sims_mean, rdk_sims_mean, morgan_sims_mean, fcd_mean], stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# Set your OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "def get_gpt_response(text, prompt,model_name = \"gpt-4o-2024-05-13\",):\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert organic chemist.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"extract the answer from the {text}, given the question is {prompt} \\n\" + 'Output:\\n'}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    ).choices[0].message.content\n",
    "    return response\n",
    "\n",
    "\n",
    "def generate_response_by_gpt(text, prompt, model_engine = \"gpt-4o-2024-05-13\"):\n",
    "    # time.sleep(1)\n",
    "    sys_prompt = f\"extract the answer from the text, given the question is {prompt} \\n\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model_engine, temperature=0.2, n=1, \n",
    "        messages=[{\"role\": \"user\", \"content\": sys_prompt + 'Output:\\n'}])\n",
    "    message = completion.choices\n",
    "    message = [i.message.content.strip() for i in message]\n",
    "    return message[0]\n",
    "F1s = []\n",
    "for i in range(0, 3):\n",
    "    details_df = pd.read_csv(f'/home/kguo2/PycharmProjects/spectrumLM/code/step3_eval/_claude-3_generated_responses_{i}.csv')\n",
    "    results = []\n",
    "    answer = []\n",
    "\n",
    "    for _, row in details_df.iterrows():\n",
    "        # text = row['Generated Response']\n",
    "        # prompt = row['question']\n",
    "        # result = get_gpt_response(text, prompt)\n",
    "        # results.append(result)\n",
    "        answer.append(row['Answer'])\n",
    "        results.append([1 if str(row['Answer']) in row['Generated Response'] else 0])\n",
    "    # results = [int(i) if len(i) == 1 else -1 for i in results]\n",
    "    # results = [1 if i == r else 0 for i, r in zip(results, answer)]\n",
    "    answer = [1] * len(answer)\n",
    "    f1 = f1_score(answer, results)\n",
    "    print(f1)\n",
    "    F1s.append(f1)\n",
    "\n",
    "print(np.array(F1s).mean())\n",
    "np.array(F1s).std()\n",
    "\n",
    "# np.array(accs).mean()\n",
    "# np.array(accs).std()\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
