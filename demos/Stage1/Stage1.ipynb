{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print (os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:06:12.418248844Z",
     "start_time": "2024-05-20T22:06:12.343315076Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"questions_answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:06:14.579942438Z",
     "start_time": "2024-05-20T22:06:14.459839893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Molecule Index</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>cls</th>\n",
       "      <th>Formula</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>CCCCC1=CC=CC=C1</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C10H14</td>\n",
       "      <td>Could the molecule with the formula C10H14 hav...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>98</td>\n",
       "      <td>CCCCOCCCC</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C8H18O</td>\n",
       "      <td>Could the molecule with the formula C8H18O hav...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>97</td>\n",
       "      <td>CCC[N+](=O)[O-]</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C3H7NO2</td>\n",
       "      <td>Could the molecule with the formula C3H7NO2 ha...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>96</td>\n",
       "      <td>C1=CC=C(C=C1)CCCBr</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C9H11Br</td>\n",
       "      <td>Could the molecule with the formula C9H11Br ha...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>95</td>\n",
       "      <td>C[C@H]([C@@H](C(=O)O)N)O</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C4H9NO3</td>\n",
       "      <td>Could the molecule with the formula C4H9NO3 ha...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6185</th>\n",
       "      <td>175</td>\n",
       "      <td>CC1C2=CC=CC=C2C3=CC=CC=C13</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C14H12</td>\n",
       "      <td>Could the molecule with the formula C14H12 hav...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6212</th>\n",
       "      <td>170</td>\n",
       "      <td>CC1(CC(=O)C2=CC=CC=C21)C</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C11H12O</td>\n",
       "      <td>Could the molecule with the formula C11H12O ha...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239</th>\n",
       "      <td>149</td>\n",
       "      <td>C1=CC=C(C=C1)COC(=O)C2=CC=CC=C2</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C14H12O2</td>\n",
       "      <td>Could the molecule with the formula C14H12O2 h...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6266</th>\n",
       "      <td>147</td>\n",
       "      <td>C(CCN)CN</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C4H12N2</td>\n",
       "      <td>Could the molecule with the formula C4H12N2 ha...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6293</th>\n",
       "      <td>144</td>\n",
       "      <td>CCOC(=O)C1CCC1</td>\n",
       "      <td>Aromatic Rings</td>\n",
       "      <td>C7H12O2</td>\n",
       "      <td>Could the molecule with the formula C7H12O2 ha...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Molecule Index                           SMILES             cls  \\\n",
       "2                 99                  CCCCC1=CC=CC=C1  Aromatic Rings   \n",
       "29                98                        CCCCOCCCC  Aromatic Rings   \n",
       "56                97                  CCC[N+](=O)[O-]  Aromatic Rings   \n",
       "83                96               C1=CC=C(C=C1)CCCBr  Aromatic Rings   \n",
       "110               95         C[C@H]([C@@H](C(=O)O)N)O  Aromatic Rings   \n",
       "...              ...                              ...             ...   \n",
       "6185             175       CC1C2=CC=CC=C2C3=CC=CC=C13  Aromatic Rings   \n",
       "6212             170         CC1(CC(=O)C2=CC=CC=C21)C  Aromatic Rings   \n",
       "6239             149  C1=CC=C(C=C1)COC(=O)C2=CC=CC=C2  Aromatic Rings   \n",
       "6266             147                         C(CCN)CN  Aromatic Rings   \n",
       "6293             144                   CCOC(=O)C1CCC1  Aromatic Rings   \n",
       "\n",
       "       Formula                                           Question Answer  \n",
       "2       C10H14  Could the molecule with the formula C10H14 hav...    Yes  \n",
       "29      C8H18O  Could the molecule with the formula C8H18O hav...     No  \n",
       "56     C3H7NO2  Could the molecule with the formula C3H7NO2 ha...     No  \n",
       "83     C9H11Br  Could the molecule with the formula C9H11Br ha...    Yes  \n",
       "110    C4H9NO3  Could the molecule with the formula C4H9NO3 ha...     No  \n",
       "...        ...                                                ...    ...  \n",
       "6185    C14H12  Could the molecule with the formula C14H12 hav...    Yes  \n",
       "6212   C11H12O  Could the molecule with the formula C11H12O ha...    Yes  \n",
       "6239  C14H12O2  Could the molecule with the formula C14H12O2 h...    Yes  \n",
       "6266   C4H12N2  Could the molecule with the formula C4H12N2 ha...     No  \n",
       "6293   C7H12O2  Could the molecule with the formula C7H12O2 ha...     No  \n",
       "\n",
       "[234 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['cls'] == 'Aromatic Rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:06:21.520796231Z",
     "start_time": "2024-05-20T22:06:21.485643669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saturation': 20, 'Saturation degree': 20, 'Aromatic Rings': 30, 'Functional Group': 30}\n",
      "Saturation\n",
      "Saturation degree\n",
      "Aromatic Rings\n",
      "Functional Group\n",
      "Sampled data saved to 'sampled_questions_answers.csv'\n",
      "{'Saturation': 20, 'Saturation degree': 20, 'Aromatic Rings': 30, 'Functional Group': 30}\n",
      "Saturation\n",
      "Saturation degree\n",
      "Aromatic Rings\n",
      "Functional Group\n",
      "Sampled data saved to 'sampled_questions_answers.csv'\n",
      "{'Saturation': 20, 'Saturation degree': 20, 'Aromatic Rings': 30, 'Functional Group': 30}\n",
      "Saturation\n",
      "Saturation degree\n",
      "Aromatic Rings\n",
      "Functional Group\n",
      "Sampled data saved to 'sampled_questions_answers.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "iteration = 3\n",
    "\n",
    "ratios = {\n",
    "        'Saturation': 0.2,\n",
    "        'Saturation degree': 0.2,\n",
    "        'Aromatic Rings': 0.3,\n",
    "        'Functional Group': 0.3\n",
    "    }\n",
    "for i in range(0, iteration):\n",
    "    total_samples = 100\n",
    "    samples_per_class = {clss: int(total_samples * ratio) for clss, ratio in ratios.items()}\n",
    "    print(samples_per_class)\n",
    "\n",
    "    sampled_data = pd.DataFrame()\n",
    "    for clss, n_samples in samples_per_class.items():\n",
    "        print(clss)\n",
    "        sampled_class_data = data[data['cls'] == clss].sample(n=n_samples, random_state=42)\n",
    "        sampled_data = pd.concat([sampled_data, sampled_class_data])\n",
    "\n",
    "\n",
    "    if len(sampled_data) < total_samples:\n",
    "        additional_samples = data[~data.index.isin(sampled_data.index)].sample(n=total_samples - len(sampled_data), random_state=42)\n",
    "        sampled_data = pd.concat([sampled_data, additional_samples])\n",
    "    os.makedirs('./data/mol_figures/mol_understanding', exist_ok=True)\n",
    "    sampled_data.to_csv(f'./data/mol_figures/mol_understanding/sampled_questions_answers_{i}.csv', index=False)\n",
    "    print(\"Sampled data saved to 'sampled_questions_answers.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:06:29.603921446Z",
     "start_time": "2024-05-20T22:06:28.571141586Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model_and_tokenizer(model_path, tokenizer_path=None, device='cuda:0', **kwargs):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        **kwargs\n",
    "    ).to(device).eval()\n",
    "\n",
    "    tokenizer_path = model_path if tokenizer_path is None else tokenizer_path\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_path,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False,\n",
    "        padding_side='left',\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'left'\n",
    "    return model, tokenizer\n",
    "\n",
    "def llm_generate(inputs, conv_template, model, tokenizer, batch_size=80, random_sample=False, max_new_tokens=256):\n",
    "    input_ids_list = []\n",
    "    for i in range(len(inputs)):\n",
    "        conv_template.append_message(conv_template.roles[0], inputs[i])\n",
    "        conv_template.append_message(conv_template.roles[1], None)\n",
    "        prompt = conv_template.get_prompt()\n",
    "        encoding = tokenizer(prompt)\n",
    "        toks = encoding.input_ids\n",
    "        input_ids = torch.tensor(toks).to(model.device)\n",
    "        input_ids_list.append(input_ids)\n",
    "        conv_template.messages = []\n",
    "    pad_tok = tokenizer.pad_token_id\n",
    "    max_input_length = max([ids.size(0) for ids in input_ids_list])\n",
    "    # Pad each input_ids tensor to the maximum length\n",
    "    padded_input_ids_list = []\n",
    "    for ids in input_ids_list:\n",
    "        pad_length = max_input_length - ids.size(0)\n",
    "        padded_ids = torch.cat([torch.full((pad_length,), pad_tok, device=model.device), ids], dim=0)\n",
    "        padded_input_ids_list.append(padded_ids)\n",
    "    input_ids_tensor = torch.stack(padded_input_ids_list, dim=0)\n",
    "    attn_mask = (input_ids_tensor != pad_tok).type(input_ids_tensor.dtype)\n",
    "    generation_config = model.generation_config\n",
    "    generation_config.max_new_tokens = max_new_tokens\n",
    "    if random_sample:\n",
    "        generation_config.do_sample = True\n",
    "        generation_config.temperature = 0.7\n",
    "        generation_config.top_p = 0.9\n",
    "    else:\n",
    "        generation_config.do_sample = False\n",
    "        generation_config.temperature = None\n",
    "        generation_config.top_p = None\n",
    "    flag = False\n",
    "    while not flag:\n",
    "        try:\n",
    "            output_ids_new = []\n",
    "            for i in range(0, len(input_ids_tensor), batch_size):\n",
    "                input_ids_tensor_batch = input_ids_tensor[i:i + batch_size]\n",
    "                attn_mask_batch = attn_mask[i:i + batch_size]\n",
    "                output_ids_batch = model.generate(input_ids_tensor_batch,\n",
    "                                                  attention_mask=attn_mask_batch,\n",
    "                                                  generation_config=generation_config,\n",
    "                                                  pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "                for j in range(len(output_ids_batch)):\n",
    "                    output_ids_new.append(output_ids_batch[j][max_input_length:])\n",
    "            flag = True\n",
    "        # except cuda out of memory error\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            batch_size = batch_size // 2\n",
    "    gen_strs = [tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output_ids_new]\n",
    "    return gen_strs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## few-shot evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:06:46.627827184Z",
     "start_time": "2024-05-20T22:06:45.878014750Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import os\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "# Set your OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ['ANTHROPIC_API_KEY'] = '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'\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "As an expert organic chemist, your task is to analyze and determine the potential structures that can be derived from a given molecular formula.\\\n",
    "Utilize your knowledge to systematically explore and identify plausible structural configurations based on the formula provided and answer the question.\n",
    "\"\"\"\n",
    "cache_dir = \"/scratch365/kguo2/TRANS_cache/\"\n",
    "\n",
    "def get_llm_response(model_name,model,prompt):\n",
    "    if 'gpt' in model_name:\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        client = OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert organic chemist.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            temperature=0.7\n",
    "        ).choices[0].message.content\n",
    "    elif 'claude' in model_name:\n",
    "        client = anthropic.Anthropic()\n",
    "        response = client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        conv_template = get_conversation_template(model_paths[model_name])\n",
    "        conv_template.system_prompt = \"You are an expert organic chemist.\"\n",
    "        response = llm_generate(prompt, conv_template, model, tokenizer, batch_size=10, random_sample=True, max_new_tokens=1024)\n",
    "        \n",
    "    return response\n",
    "\n",
    "def generate_prompt(prompt):\n",
    "    if row['cls'] == 'Saturation':\n",
    "        instruction = \"Analyze the problem step-by-step internally, but do not include the analysis in your output. Provide only a very short answer with the exact result. Respond with ‘Yes’ or ‘No’ to indicate whether the molecule could potentially be saturated.\"\n",
    "    elif row['cls'] == 'Saturation degree':\n",
    "        instruction = \"Analyze the problem step-by-step internally, but do not include the analysis in your output. Provide only a very short answer with the exact result. Respond with a number from 0 to 13 to indicate the Degree of Unsaturation of the molecule.\"\n",
    "    elif row['cls'] == 'Aromatic Rings':\n",
    "        instruction =  \"Analyze the problem step-by-step internally, but do not include the analysis in your output. Provide only a very short answer with the exact result. Respond with ‘Yes’ or ‘No’ to indicate whether the molecule could have aromatic rings.\"\n",
    "    else:\n",
    "        instruction = \"Analyze the problem step-by-step internally, but do not include the analysis in your output. Provide only a very short answer with the exact result. Respond with ‘Yes’ or ‘No’ to indicate whether the molecule could potentially contain the functional group\"\n",
    "    prompt = prompt + instruction + \"\\n\" + row['Question'] + \"\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def is_open_source(model_name):\n",
    "    if 'claude' in model_name or 'gemini' in model_name or 'gpt' in model_name:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "llama_path = model_paths['llama3']\n",
    "print(llama_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = load_model_and_tokenizer(llama_path,\n",
    "#                                             low_cpu_mem_usage=True,\n",
    "#                                             use_cache=False,\n",
    "#                                             device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_open_source(\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch365/kguo2/TRANS_cache/\n"
     ]
    }
   ],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = \"/scratch365/kguo2/TRANS_cache/\"\n",
    "os.environ['HF_HUB_CACHE'] = \"/scratch365/kguo2/TRANS_cache/\"\n",
    "os.environ['HF_HOME'] = \"/scratch365/kguo2/TRANS_cache/\"\n",
    "os.environ['HUGGINGFACE_HUB_CACHE']=\"/scratch365/kguo2/TRANS_cache/\"\n",
    "os.environ['HF_DATASETS_CACHE']=\"/scratch365/kguo2/TRANS_cache/\"\n",
    "print(os.environ['HF_HUB_CACHE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: huggingface-cli: command not found\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f13a9c9f4d4e1eafd29f64edd8f715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "iteration = 3\n",
    "model_names = ['llama3']\n",
    "\n",
    "for model_name in model_names:\n",
    "    for i in range(0, iteration):\n",
    "        question_data = pd.read_csv(f'./data/mol_figures/mol_understanding/sampled_questions_answers_{i}.csv')\n",
    "        if is_open_source(model_name) and i == 0:\n",
    "            print(model_name)\n",
    "            model, tokenizer = load_model_and_tokenizer(model_paths[model_name],\n",
    "                                                        cache_dir=cache_dir,\n",
    "                                                        low_cpu_mem_usage=True,\n",
    "                                                        use_cache=False,\n",
    "                                                        device='cuda'\n",
    "                                                         )\n",
    "            print('running')\n",
    "        data_frame = pd.DataFrame(columns=[\"question\", \"cls\", \"Answer\", \"Generated Response\"])\n",
    "        prompts = []\n",
    "        for index, row in question_data.iterrows():\n",
    "            prompt = \"\"\"\n",
    "                    As an expert organic chemist, your task is to analyze and determine the potential structures that can be derived from a given molecular formula.\\\n",
    "                    Utilize your knowledge to systematically explore and identify plausible structural configurations based on the formula provided and answer the question.\n",
    "                    \"\"\"\n",
    "            prompt = generate_prompt(prompt)\n",
    "            prompts.append(prompt)\n",
    "        if is_open_source(model_name):\n",
    "            generated_responses = get_llm_response(model_name,model, prompts)\n",
    "            for index, row in question_data.iterrows():\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'], row['Answer'], generated_responses[index]]\n",
    "        else:\n",
    "            for index, row in question_data.iterrows():\n",
    "                generated_response = get_llm_response(model_name,model, prompt)\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'], row['Answer'], generated_response]\n",
    "        data_frame.to_csv(f'./data/mol_figures/mol_understanding/{model_name}_generated_responses_{i}.csv', index=False)\n",
    "    del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "acc and f1 score for 'Saturation': 0.2, 'Aromatic Rings ': 0.3, 'Functional Group': 0.3\n",
    "must contain for 'Saturation degree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:08:22.269442330Z",
     "start_time": "2024-05-20T22:08:21.933303196Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    categories = ['Saturation', 'Aromatic Rings', 'Functional Group']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        # Filter data for the current category\n",
    "        category_data = df[df['cls'] == category]\n",
    "        \n",
    "        # Compute accuracy\n",
    "        answer = [1 if ans == 'Yes' else 0 for ans in category_data['Answer']]\n",
    "    \n",
    "        category_data['Generated Response'] = category_data['Generated Response'].apply(str)\n",
    "\n",
    "\n",
    "        generated_response = [1 if 'Yes' in ans else 0 for ans in category_data['Generated Response']]\n",
    "        accuracy = accuracy_score(answer, generated_response)\n",
    "        \n",
    "        # Compute F1 score\n",
    "        f1 = f1_score(answer, generated_response, average='macro')\n",
    "        \n",
    "        results[category] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_number_from_string(s):\n",
    "    # Use regular expressions to find all numbers in the string\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', s)\n",
    "    if numbers:\n",
    "        # Convert the first number found to a float and then to an integer\n",
    "        return int(float(numbers[0]))\n",
    "    return None\n",
    "\n",
    "def must_include(str1, str2):\n",
    "    # Extract the number from str2\n",
    "    number = extract_number_from_string(str2)\n",
    "    \n",
    "    if number is not None:\n",
    "        # Formulate the pattern \"is x\" where x is the extracted number\n",
    "        pattern = f\"{number}\"\n",
    "        \n",
    "        # Check if the pattern is present in str1\n",
    "        return 1 if pattern in str1 else 0\n",
    "    \n",
    "def evaluate_saturation_degree(df):\n",
    "    degree_data = df[df['cls'] == 'Saturation degree']\n",
    "    \n",
    "    correct_count = 0\n",
    "    \n",
    "    for index, row in degree_data.iterrows():\n",
    "            # row['Generated Response'] = row['Generated Response'].apply(str)\n",
    "            generated_response = row['Generated Response']\n",
    "            try:\n",
    "                row['Generated Response'] = row['Generated Response'].apply(str)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            answer = row['Answer']\n",
    "            must_include_flag = must_include(generated_response, answer)\n",
    "        \n",
    "            correct_count += must_include_flag\n",
    "    # print(\"correct\",correct_count)\n",
    "    accuracy = correct_count / len(degree_data)\n",
    "    return {'Saturation degree': {'Accuracy': accuracy}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:08:29.805756669Z",
     "start_time": "2024-05-20T22:08:29.710502567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3\n",
      "{'Saturation': {'Accuracy': 0.35, 'F1 Score': 0.25925925925925924}, 'Aromatic Rings': {'Accuracy': 0.7, 'F1 Score': 0.6827262044653349}, 'Functional Group': {'Accuracy': 0.5666666666666667, 'F1 Score': 0.5238095238095237}}\n",
      "correct 0\n",
      "{'Saturation degree': {'Accuracy': 0.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/566636.1.gpu/ipykernel_682371/1020366446.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_data['Generated Response'] = category_data['Generated Response'].apply(str)\n",
      "/tmp/566636.1.gpu/ipykernel_682371/1020366446.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_data['Generated Response'] = category_data['Generated Response'].apply(str)\n",
      "/tmp/566636.1.gpu/ipykernel_682371/1020366446.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_data['Generated Response'] = category_data['Generated Response'].apply(str)\n"
     ]
    }
   ],
   "source": [
    "model_names = ['llama3']\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    generated_answers = pd.read_csv(f'./data/mol_figures/mol_understanding/{model_name}_generated_responses_{0}.csv')\n",
    "    results = evaluate_responses(generated_answers)\n",
    "    print(results)\n",
    "    sa_degree_results = evaluate_saturation_degree(generated_answers)\n",
    "    print(sa_degree_results)\n",
    "# results = evaluate_responses(generated_answers)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:08:40.239459943Z",
     "start_time": "2024-05-20T22:08:40.196717163Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "model_names = ['claude']\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    sa_f1 = []\n",
    "    sa_acc = []\n",
    "    ar_acc = []\n",
    "    ar_f1 = []\n",
    "    fg_acc = []\n",
    "    fg_f1 = []\n",
    "    matchs = []\n",
    "    for i in range(0, 3):\n",
    "        generated_answers = pd.read_csv(f'./data/mol_figures/mol_understanding/{model_name}_generated_responses_{i}.csv')\n",
    "        results = evaluate_responses(generated_answers)\n",
    "        sa_degree_results = evaluate_saturation_degree(generated_answers)\n",
    "        sa_f1.append(results['Saturation']['F1 Score'])\n",
    "        sa_acc.append(results['Saturation']['Accuracy'])\n",
    "        ar_acc.append(results['Aromatic Rings']['Accuracy'])\n",
    "        ar_f1.append(results['Aromatic Rings']['F1 Score'])\n",
    "        fg_acc.append(results['Functional Group']['Accuracy'])\n",
    "        fg_f1.append(results['Functional Group']['F1 Score'])\n",
    "        matchs.append(sa_degree_results['Saturation degree']['Accuracy'])\n",
    "    sa_f1 = np.array(sa_f1)\n",
    "    sa_acc = np.array(sa_acc)\n",
    "    ar_acc = np.array(ar_acc)\n",
    "    ar_f1 = np.array(ar_f1)\n",
    "    fg_acc = np.array(fg_acc)\n",
    "    fg_f1 = np.array(fg_f1)\n",
    "    matchs = np.array(matchs)\n",
    "    print(sa_f1.mean(),ar_f1.mean(),fg_f1.mean())\n",
    "    print(sa_f1.std(),ar_f1.std(),fg_f1.std())\n",
    "    print(sa_acc.mean(),ar_acc.mean(),fg_acc.mean(),matchs.mean())\n",
    "    print(sa_acc.std(),ar_acc.std(),fg_acc.std(),matchs.std())\n",
    "    print(\"f1 mean\",(sa_f1.mean() * 0.2 + ar_f1.mean() * 0.3 + fg_f1.mean() * 0.3 ) / 0.8)\n",
    "    print(\"f1 std\",(sa_f1.std() * 0.2 + ar_f1.std() * 0.3 + fg_f1.std() * 0.3 ) / 0.8)\n",
    "    print(\"acc mean\",(sa_acc.mean() * 0.2 + ar_acc.mean() * 0.3 + fg_acc.mean() * 0.3 ) / 0.8)\n",
    "    print(\"acc std\",(sa_acc.std() * 0.2 + ar_acc.std() * 0.3 + fg_acc.std() * 0.3 ) / 0.8)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:08:47.222281092Z",
     "start_time": "2024-05-20T22:08:47.218690554Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_prompt = \"\"\"Let’s think through the steps we need to solve this problem: as an expert organic chemist, your task is to analyze and determine the potential structures that can be derived from a given molecular formula. Utilize your knowledge to systematically explore and identify plausible structural configurations based on the formula provided and answer the question.\"\"\"\n",
    "\n",
    "def generate_prompt_cot(cot_response,prompt,row):\n",
    "    cot = \"Here is the reasoning for the answer:\" + cot_response\n",
    "\n",
    "    \n",
    "    if row['cls'] == 'Saturation':\n",
    "        instruction = \"Analyze the problem step-by-step internally, but do not include the analysis in your output. Provide only a very short answer with the exact result. Respond with ‘Yes’ or ‘No’ to indicate whether the molecule could potentially be saturated.\"\n",
    "    elif row['cls'] == 'Saturation degree':\n",
    "        instruction = \"Analyze the problem step-by-step internally, but do not include the analysis in your output. Provide only a very short answer with the exact result. Respond with a number from 0 to 13 to indicate the Degree of Unsaturation of the molecule.\"\n",
    "    elif row['cls'] == 'Aromatic Rings':\n",
    "        instruction =  \"Analyze the problem step-by-step internally, but do not include the analysis in your output. Provide only a very short answer with the exact result. Respond with ‘Yes’ or ‘No’ to indicate whether the molecule could have aromatic rings.\"\n",
    "    else:\n",
    "        instruction = \"Analyze the problem step-by-step internally, but do not include the analysis in your output. Provide only a very short answer with the exact result. Respond with ‘Yes’ or ‘No’ to indicate whether the molecule could potentially contain the functional group\"\n",
    "\n",
    "    prompt = cot + \"\\n\" + instruction + \"\\n\" + row['Question'] + \"\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: huggingface-cli: command not found\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:24:37.394462273Z",
     "start_time": "2024-05-20T22:21:39.295346799Z"
    }
   },
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "model_names = ['model/gemini']\n",
    "# models = [\"gpt-3.5-turbo-0125\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    for i in range(0, iteration):\n",
    "        if is_open_source(model_name) and i == 0:\n",
    "            model, tokenizer = load_model_and_tokenizer(model_paths[model_name],\n",
    "                                                        low_cpu_mem_usage=True,\n",
    "                                                        use_cache=False,\n",
    "                                                        cache_dir=cache_dir,\n",
    "                                                        device='cuda')\n",
    "        question_data = pd.read_csv(f'./data/mol_figures/mol_understanding/sampled_questions_answers_{i}.csv')\n",
    "        # question_data = question_data.sample(n=10, random_state=42)\n",
    "        data_frame = pd.DataFrame(columns=[\"question\", \"cls\",\"cot\", \"Answer\", \"Generated Response\"])\n",
    "        if is_open_source(model_name):\n",
    "            print(\"running\")\n",
    "            prompts = []\n",
    "            for index, row in question_data.iterrows():\n",
    "                prompt = cot_prompt + \"\\n\"  + row['Question'] + \"\\n\"\n",
    "                prompts.append(prompt)\n",
    "            cot_responses = get_llm_response(model_name,model, prompts)\n",
    "            new_prompts = []\n",
    "            for index, row in question_data.iterrows():\n",
    "                row['cot'] = cot_responses[index]\n",
    "                new_prompt = generate_prompt_cot(cot_responses[index], prompts[index], row)\n",
    "                new_prompts.append(new_prompt)\n",
    "            generated_responses = get_llm_response(model_name,model, prompts)\n",
    "            for index, row in question_data.iterrows():\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'], cot_responses[index],row['Answer'], generated_responses[index]]\n",
    "        else:\n",
    "            for index, row in question_data.iterrows():\n",
    "                prompt = cot_prompt + \"\\n\"  + row['Question'] + \"\\n\"\n",
    "                cot_response = get_llm_response(model_name,_, prompts)\n",
    "                row['cot'] = cot_response\n",
    "                prompt = generate_prompt_cot(cot_response, prompt,row)\n",
    "                generated_response = get_llm_response(model_name,_, prompt)\n",
    "                data_frame.loc[len(data_frame)] = [row['Question'], row['cls'],row['cot'],row['Answer'], generated_response]\n",
    "        data_frame.to_csv(f'./data/mol_figures/mol_understanding/cot_{model_name}_generated_responses_{i}.csv', index=False)    \n",
    "    del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T22:24:42.125903394Z",
     "start_time": "2024-05-20T22:24:42.109320412Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "model_names = ['claude']\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    sa_f1 = []\n",
    "    sa_acc = []\n",
    "    ar_acc = []\n",
    "    ar_f1 = []\n",
    "    fg_acc = []\n",
    "    fg_f1 = []\n",
    "    matchs = []\n",
    "    for i in range(0, 2):\n",
    "        generated_answers = pd.read_csv(f'./data/mol_figures/mol_understanding/cot_{model_name}_generated_responses_{i}.csv')\n",
    "        results = evaluate_responses(generated_answers)\n",
    "        # sa_degree_results = evaluate_saturation_degree(generated_answers)\n",
    "        sa_f1.append(results['Saturation']['F1 Score'])\n",
    "        sa_acc.append(results['Saturation']['Accuracy'])\n",
    "        ar_acc.append(results['Aromatic Rings']['Accuracy'])\n",
    "        ar_f1.append(results['Aromatic Rings']['F1 Score'])\n",
    "        fg_acc.append(results['Functional Group']['Accuracy'])\n",
    "        fg_f1.append(results['Functional Group']['F1 Score'])\n",
    "        # matchs.append(sa_degree_results['Saturation degree']['Accuracy'])\n",
    "    sa_f1 = np.array(sa_f1)\n",
    "    sa_acc = np.array(sa_acc)\n",
    "    ar_acc = np.array(ar_acc)\n",
    "    ar_f1 = np.array(ar_f1)\n",
    "    fg_acc = np.array(fg_acc)\n",
    "    fg_f1 = np.array(fg_f1)\n",
    "    matchs = np.array(matchs)\n",
    "    print(sa_f1.mean(),ar_f1.mean(),fg_f1.mean())\n",
    "    print(sa_f1.std(),ar_f1.std(),fg_f1.std())\n",
    "    # print(sa_acc.mean(),ar_acc.mean(),fg_acc.mean(),matchs.mean())\n",
    "    # print(sa_acc.std(),ar_acc.std(),fg_acc.std(),matchs.std())\n",
    "    # # print(\"f1 mean\",(sa_f1.mean() * 0.2 + ar_f1.mean() * 0.3 + fg_f1.mean() * 0.3 ) / 0.8)\n",
    "    # # print(\"f1 std\",(sa_f1.std() * 0.2 + ar_f1.std() * 0.3 + fg_f1.std() * 0.3 ) / 0.8)\n",
    "    # # print(\"acc mean\",(sa_acc.mean() * 0.2 + ar_acc.mean() * 0.3 + fg_acc.mean() * 0.3 ) / 0.8)\n",
    "    # # print(\"acc std\",(sa_acc.std() * 0.2 + ar_acc.std() * 0.3 + fg_acc.std() * 0.3 ) / 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
